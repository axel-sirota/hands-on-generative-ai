{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eGC5zBa03SRC"
   },
   "source": [
    "# Using GloVe Embedding\n",
    "\n",
    "Â© Data Trainers LLC. GPL v 3.0.\n",
    "\n",
    "**Author:** Axel Sirota\n",
    "\n",
    "\n",
    "In this notebook we will leverage Standford's GloVe vectors which is a pretrained embedding on 1.4B Tweets.\n",
    "\n",
    "Take it easy and pay attention to the main differences with before, and the non-trainable parameters. Finally, check how accurate the results now are.\n",
    "You can run this lab both locally or in Colab.\n",
    "\n",
    "- To run in Colab just go to `https://colab.research.google.com`, sign-in and you upload this notebook. Colab has GPU access for free.\n",
    "- To run locally just run `jupyter notebook` and access the notebook in this lab. You would need to first install the requirements in `requirements.txt`\n",
    "\n",
    "Follow the instructions. Good luck!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "75x9mZNIbPAp"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YUsYzWxfT8o-"
   },
   "outputs": [],
   "source": [
    "!pip install textblob 'keras-nlp' 'keras-preprocessing' 'gensim==4.2.0' np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h3XUwgb0UBut"
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Lambda\n",
    "import np_utils\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from textblob import TextBlob, Word\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.initializers import Constant\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import warnings\n",
    "import nltk\n",
    "\n",
    "TRACE = False\n",
    "embedding_dim = 100\n",
    "epochs=2\n",
    "batch_size = 500\n",
    "BATCH = True\n",
    "\n",
    "def set_seeds_and_trace():\n",
    "  os.environ['PYTHONHASHSEED'] = '0'\n",
    "  np.random.seed(42)\n",
    "  tf.random.set_seed(42)\n",
    "  random.seed(42)\n",
    "  if TRACE:\n",
    "    tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "def set_session_with_gpus_and_cores():\n",
    "  cores = multiprocessing.cpu_count()\n",
    "  gpus = len(tf.config.list_physical_devices('GPU'))\n",
    "  config = tf.compat.v1.ConfigProto( device_count = {'GPU': gpus  , 'CPU': cores} , intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "  sess = tf.compat.v1.Session(config=config)\n",
    "  tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "set_seeds_and_trace()\n",
    "set_session_with_gpus_and_cores()\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('punkt')\n",
    "textblob_tokenizer = lambda x: TextBlob(x).words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tx3hZd6gUImG"
   },
   "outputs": [],
   "source": [
    "%%writefile get_data.sh\n",
    "if [ ! -f yelp.csv ]; then\n",
    "  wget -O yelp.csv https://www.dropbox.com/s/xds4lua69b7okw8/yelp.csv?dl=0\n",
    "fi\n",
    "\n",
    "if [ ! -f glove.6B.100d.txt ]; then\n",
    "  wget -O glove.6B.100d.txt https://www.dropbox.com/s/dl1vswq2sz5f1ws/glove.6B.100d.txt?dl=0\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PfyMUL8nXRYP"
   },
   "outputs": [],
   "source": [
    "!bash get_data.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ckCWLwTXVa0"
   },
   "outputs": [],
   "source": [
    "path = './yelp.csv'\n",
    "yelp = pd.read_csv(path)\n",
    "# Create a new DataFrame that only contains the 5-star and 1-star reviews to have extremes.\n",
    "yelp_best_worst = yelp[(yelp.stars==5) | (yelp.stars==1)]\n",
    "X = yelp_best_worst.text\n",
    "y = yelp_best_worst.stars.map({1:0, 5:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yV8JS-8harja"
   },
   "outputs": [],
   "source": [
    "corpus = [sentence for sentence in X.values if type(sentence) == str and len(TextBlob(sentence).words) > 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B__Of4dUXXBU"
   },
   "outputs": [],
   "source": [
    "path_to_glove_file = \"./glove.6B.100d.txt\"\n",
    "embeddings_index = {}\n",
    "# Construct a function that fills the embedding_index dict for every word in the GloVe file with its coefficients.\n",
    "# HELP: For that iterate over the Glove file (hint: check that file to view its structure first!), split the word from the numbers, and populate the dictionary with the word and the numbers as a numpy array.\n",
    "# Hint2: check np.fromstring\n",
    "# FILL\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fygJjt56Xhup"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "tokenized_corpus = tokenizer.texts_to_sequences(corpus)\n",
    "nb_samples = sum(len(s) for s in corpus)\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pfkDahA5XnzL"
   },
   "outputs": [],
   "source": [
    "print(f'First 5 corpus items are {corpus[:5]}')\n",
    "print(f'Length of corpus is {len(corpus)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bp6wXMtvXoJ4"
   },
   "outputs": [],
   "source": [
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "# Create a loop such that for every word in the vocabulary, if it exists in the Glove embedding, then set for that word (that means that index) the tensor of the Glove Embedding. Otherwise fill with 0\n",
    "# FILL\n",
    "\n",
    "# In the end, the embedding matrix should have, for every word of our tokenizer that exists in GloVe, the tensor representation of that word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T1nRW7STX3sr"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add()  # Add the new embedding and set it as non-trainable (although you could fine-tune it if you prefer)\n",
    "model.add()  # Add the same Lambda as before to average out the words dimension\n",
    "model.add()  # Add a Dense layer to classify words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CmuPaNl4YEc7"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S6Ut0I9Q3SRM"
   },
   "source": [
    "Notice the Non-trainable parameters! What we are doing is just training the softmax based on correct embeddings. This is called fine tuning the embedding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_433LAsvYHW-"
   },
   "outputs": [],
   "source": [
    "def generate_data(corpus, vocab_size, window_size=2, sentence_batch_size=10,  batch_size=250):\n",
    "    np.random.shuffle(np.array(corpus))\n",
    "    number_of_sentence_batches = (len(corpus) // sentence_batch_size) + 1\n",
    "    for batch in range(number_of_sentence_batches):\n",
    "        lower_end = batch*batch_size\n",
    "        upper_end = (batch+1)*batch_size if batch+1 < number_of_sentence_batches else len(corpus)\n",
    "        mini_batch_size = upper_end - lower_end\n",
    "        maxlen = window_size*2\n",
    "        X = []\n",
    "        Y = []\n",
    "        for review_id, words in enumerate(corpus[lower_end:upper_end]):\n",
    "            L = len(words)\n",
    "            for index, word in enumerate(words):\n",
    "                contexts = []\n",
    "                labels   = []\n",
    "                s = index - window_size\n",
    "                e = index + window_size + 1\n",
    "\n",
    "                contexts.append([words[i] for i in range(s, e) if 0 <= i < L and i != index])\n",
    "                labels.append(word)\n",
    "\n",
    "                x = pad_sequences(contexts, maxlen=maxlen)\n",
    "                y = to_categorical(labels, vocab_size)\n",
    "                X.append(x)\n",
    "                Y.append(y)\n",
    "        X = tf.constant(X)\n",
    "        Y = tf.constant(Y)\n",
    "        number_of_batches = len(X) // batch_size\n",
    "        for real_batch in range(number_of_batches):\n",
    "          lower_end = batch*batch_size\n",
    "          upper_end = (batch+1)*batch_size\n",
    "          batch_X = tf.squeeze(X[lower_end:upper_end])\n",
    "          batch_Y = tf.squeeze(Y[lower_end:upper_end])\n",
    "          yield (batch_X, batch_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t8qf2WhiYwOJ"
   },
   "outputs": [],
   "source": [
    "# Re implement the method\n",
    "def fit_model():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UQz1WZF3atX3"
   },
   "outputs": [],
   "source": [
    "fit_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "xUV-Cwz23SRN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "Kr_mqRRB3SRN"
   },
   "outputs": [],
   "source": [
    "with open('./vectors.txt' ,'w') as f:\n",
    "    f.write('{} {}\\n'.format(vocab_size-1, embedding_dim))\n",
    "    vectors = model.get_weights()[0]\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        str_vec = ' '.join(map(str, list(vectors[i, :])))\n",
    "        f.write('{} {}\\n'.format(word, str_vec))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "_vM_y0Fc3SRO"
   },
   "outputs": [],
   "source": [
    "w2v = gensim.models.KeyedVectors.load_word2vec_format('./vectors.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CC5fH0WE3SRO"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "hNK0Gaq33SRO"
   },
   "outputs": [],
   "source": [
    "w2v.most_similar(positive=['pizza'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "VGn4aNYH3SRO"
   },
   "outputs": [],
   "source": [
    "w2v.most_similar(positive=['grape'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bkz7L2Lo3SRP"
   },
   "source": [
    "Do you notice the difference in the accuracy? For any task first search if there are any pretrained models to use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fMuG5oPdau7j"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": [],
   "gpuClass": "premium"
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}