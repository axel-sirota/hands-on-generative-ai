{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "# News Classification with Feed Forward Neural Networks (Multi-Layer Perceptrons)\n",
    "\n",
    "Â© Data Trainers LLC. GPL v 3.0.\n",
    "\n",
    "**Author:** Axel Sirota\n",
    "\n",
    "\n",
    "In this notebook we will use the cnn headline dataset that has 130000 news titles and their category between: Sports, Business, Sci-Tec and World.\n",
    "\n",
    "Take it slow and notice we will leverage the embeddings we learned before, and notice the last layer will need a softmax and as many cells as categories we have.\n",
    "\n",
    "You can run this lab both locally or in Colab.\n",
    "\n",
    "- To run in Colab just go to `https://colab.research.google.com`, sign-in and you upload this notebook. Colab has GPU access for free.\n",
    "- To run locally just run `jupyter notebook` and access the notebook in this lab. You would need to first install the requirements in `requirements.txt`\n",
    "\n",
    "Follow the instructions. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DEqSAWGgNeRx"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-IY2pTPBPir1"
   },
   "outputs": [],
   "source": [
    "!pip install textblob 'keras-nlp' 'keras-preprocessing' 'gensim==4.2.0' np_utils swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5gZklVraP2vn"
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Lambda, ELU, Conv1D, MaxPooling1D, Dropout\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from textblob import TextBlob, Word\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.initializers import Constant\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import warnings\n",
    "import nltk\n",
    "import pickle\n",
    "from tensorflow.nn import leaky_relu\n",
    "\n",
    "import re\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from textblob import TextBlob\n",
    "\n",
    "TRACE = False\n",
    "embedding_dim = 300\n",
    "epochs=100\n",
    "batch_size = 250\n",
    "corpus_size=25000\n",
    "BATCH = True\n",
    "\n",
    "def set_seeds_and_trace():\n",
    "  os.environ['PYTHONHASHSEED'] = '0'\n",
    "  np.random.seed(42)\n",
    "  tf.random.set_seed(42)\n",
    "  random.seed(42)\n",
    "  if TRACE:\n",
    "    tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "def set_session_with_gpus_and_cores():\n",
    "  cores = multiprocessing.cpu_count()\n",
    "  gpus = len(tf.config.list_physical_devices('GPU'))\n",
    "  config = tf.compat.v1.ConfigProto( device_count = {'GPU': gpus  , 'CPU': cores} , intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "  sess = tf.compat.v1.Session(config=config)\n",
    "  tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "set_seeds_and_trace()\n",
    "set_session_with_gpus_and_cores()\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('punkt')\n",
    "textblob_tokenizer = lambda x: TextBlob(x).words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I0DZp9-YP8Fa"
   },
   "outputs": [],
   "source": [
    "%%writefile get_data.sh\n",
    "if [ ! -f news.csv ]; then\n",
    "  wget -O news.csv https://www.dropbox.com/s/352x7xzivf60zgc/news.csv?dl=0\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Arc-2X3AS2dI"
   },
   "outputs": [],
   "source": [
    "!bash get_data.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qIsjpoQMS5tT"
   },
   "outputs": [],
   "source": [
    "path = './news.csv'\n",
    "news_pre = pd.read_csv(path, header=0).sample(n=corpus_size).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e48igNipS-7m"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text, should_join=True):\n",
    "    # Use the tokenizer to tokenize into words, lowercase them, remove punctuation, and finally use gensim.utils.simple_preprocess(text)\n",
    "    text = None\n",
    "    if should_join:\n",
    "      return ' '.join(gensim.utils.simple_preprocess(text))\n",
    "    else:\n",
    "      return gensim.utils.simple_preprocess(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AmuzxBe0S-45"
   },
   "outputs": [],
   "source": [
    "import swifter\n",
    "# Use swifter to apply the preprocessin and save that pandas series to a file\n",
    "news = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BI4j9GohS-1k"
   },
   "outputs": [],
   "source": [
    "\n",
    "class MyCorpus:\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        corpus_path = 'news_processed.csv'\n",
    "        for line in open(corpus_path):\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            yield preprocess_text(line, should_join=False)\n",
    "\n",
    "\n",
    "word2vec_model = None\n",
    "# Get a word2vec model using gensim.models and passing the sentences using MyCorpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g02Wjl0NS-y2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "weights = None  # Get the weights of the model (the embedding) and convert to tensor. Hint: Check word2vec_model.wv\n",
    "vocab_size = None  # get vocab size from index_to_key in word2vec_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qS0pq9sbO7hy"
   },
   "outputs": [],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "news_preprocessed = pd.DataFrame()\n",
    "news_preprocessed['label'] = news_pre.category.map({'Business': 0, 'Sports': 1, 'Sci/Tech': 2, 'World': 3})\n",
    "news_preprocessed['title'] = news\n",
    "news_preprocessed"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "NeGhKfUu5OM3"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Fsl1VwnVJoz"
   },
   "outputs": [],
   "source": [
    "def get_maximum_review_length(df):\n",
    "    maximum = 0\n",
    "    for ix, row in df.iterrows():\n",
    "        candidate = len(textblob_tokenizer(row.title))\n",
    "        if candidate > maximum:\n",
    "            maximum = candidate\n",
    "    return maximum\n",
    "\n",
    "\n",
    "maximum = get_maximum_review_length(news_preprocessed)   # Since 2 titles may have different number of words, we have to find the max length and fill with 0s if a title is shorter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7cFi0e_sVJmh"
   },
   "outputs": [],
   "source": [
    "X = np.zeros((len(news_preprocessed), maximum))   # Here we do what we said above\n",
    "# Iterate through the news df and for every word, if it exists in the word2vec model, put into X for that review and that word the index of the embedding (check index_to_key)\n",
    "# HINT: to iterate through a column of a pandas dataframe you do:\n",
    "\n",
    "# for index, value in df.iterrows():\n",
    "#      #do something\n",
    "\n",
    "# FILL\n",
    "y = news_preprocessed.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HspIJVUBVJkF"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train = tf.constant(X_train)\n",
    "X_test = tf.constant(X_test)\n",
    "# Convert y_train and y_test from an array of values between 0-3 to a one hot matrix tensor\n",
    "y_train = None\n",
    "y_test = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FzrIQ4G0VJhh"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add()  # Add an Embedding layer with weights being the rweights variable and trainable as False. The embedding dimension should be embedding_dim\n",
    "model.add()  # Add a couple of Dense Layers with RELU or leaky_relu activations. You may add Batch Norm if you want too\n",
    "model.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(None, embedding_dim,)))  # Average out the words of the sentence. The expected out is (N, D) where N is number f samples in batch and D is embedding dimension\n",
    "model.add()  # Add final Dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8T-hPDiIVJe9"
   },
   "outputs": [],
   "source": [
    "# Compile the model. Think what is the best loss to use\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XCQiCKLSVJcg"
   },
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, min_delta=0.01)\n",
    "history = None # Fit the model, use the callback above to do EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tDKdanUVa4GT"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# function for plotting loss\n",
    "def plot_metrics(train_metric, val_metric=None, metric_name=None, title=None, ylim=5):\n",
    "    plt.title(title)\n",
    "    plt.ylim(0,ylim)\n",
    "    plt.plot(train_metric,color='blue',label=metric_name)\n",
    "    if val_metric is not None: plt.plot(val_metric,color='green',label='val_' + metric_name)\n",
    "    plt.legend(loc=\"upper right\")\n",
    "\n",
    "# plot loss history\n",
    "plot_metrics(history.history['loss'], history.history['val_loss'], \"Loss\", \"Loss\", ylim=2.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g3N8INqJczF0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Test with the following two sentences:\n",
    "# - 'supercomputer will put workers jobless soon'\n",
    "# - 'patriots goes winning super bowl'\n",
    "\n",
    "x_val = np.zeros((2, maximum))\n",
    "for index, row in enumerate(['supercomputer will put workers jobless soon', 'patriots goes winning super bowl']):\n",
    "    pass\n",
    "y_val = tf.one_hot([0,1], depth=4)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}