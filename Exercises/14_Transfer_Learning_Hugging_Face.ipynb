{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"16PlWQG9Kqxy4uZKze4bYsvG555Koa7wU","timestamp":1695616735882}],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyMFX+NbaItDnLRTh9/rlit/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Transfer Learning Transformers with HuggingFace\n","\n","© Data Trainers LLC. GPL v 3.0.\n","\n","Author: Axel Sirota"],"metadata":{"id":"gzf37X_XizQQ"}},{"cell_type":"markdown","source":["HuggingFace is a company with a heavy open source philosophy that makes transformers readily available so you don't have to do what we did before for every application."],"metadata":{"id":"5MjeyOLmr-WF"}},{"cell_type":"markdown","source":["## Prep"],"metadata":{"id":"cm9GN4WNtK_d"}},{"cell_type":"code","source":["!pip install -U datasets evaluate transformers transformers[sentencepiece]"],"metadata":{"id":"C9BTEOu0PerV"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0fpgYwAtNO2T"},"outputs":[],"source":["import multiprocessing\n","import tensorflow as tf\n","from datasets import load_dataset\n","from transformers import AutoTokenizer, DataCollatorWithPadding\n","import numpy as np\n","\n","import sys\n","import keras.backend as K\n","import random\n","import os\n","import pandas as pd\n","import warnings\n","import time\n","\n","TRACE = False\n","PATIENCE = 2\n","EPOCHS = 3\n","BATCH_SIZE = 256\n","\n","def set_seeds_and_trace():\n","  os.environ['PYTHONHASHSEED'] = '0'\n","  np.random.seed(42)\n","  tf.random.set_seed(42)\n","  random.seed(42)\n","  if TRACE:\n","    tf.debugging.set_log_device_placement(True)\n","\n","def set_session_with_gpus_and_cores():\n","  cores = multiprocessing.cpu_count()\n","  gpus = len(tf.config.list_physical_devices('GPU'))\n","  config = tf.compat.v1.ConfigProto( device_count = {'GPU': gpus  , 'CPU': cores} , intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n","  sess = tf.compat.v1.Session(config=config)\n","  tf.compat.v1.keras.backend.set_session(sess)\n","\n","set_seeds_and_trace()\n","set_session_with_gpus_and_cores()\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","source":["## Tokenizing and loading the dataset\n","\n","In HuggingFace there are many models, and each has its own tokenizer. Lucky for us there is a class `AutoTokenizer` that does the heavylifting after we provide a checkpoint"],"metadata":{"id":"kQb9SHI-tNyH"}},{"cell_type":"code","source":["from datasets import load_dataset\n","from transformers import AutoTokenizer\n","import numpy as np\n","\n","raw_datasets = load_dataset(\"imdb\")  # load imdb dataset\n","raw_datasets"],"metadata":{"id":"gkzGKLjN-T9k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Notice it is a dict object with the train, test, and unsupervised datasets to play around"],"metadata":{"id":"CnDCHDZ7trYz"}},{"cell_type":"code","source":["raw_datasets['train'][0]  # Let's see the first review"],"metadata":{"id":"1Q13wuHGtqu4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["How do we know if it's positive or negative from label=0?"],"metadata":{"id":"hz7V6Iw1uEJR"}},{"cell_type":"code","source":["raw_datasets['train'].features"],"metadata":{"id":"HgQQCHo5t99k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["There it is, within features we see that the index 0 is **Negative**\n","\n","Now to tokenise the dataset we need to load the proper tokenizer for the model we care about. And the we are goin to apply it everywhere!\n","\n","After this step the tokenizer converts the text into a Tensor of ids, each representing a diferent word in the BERT vocabulary"],"metadata":{"id":"C7p9f6uYuDNb"}},{"cell_type":"code","source":["checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","tokenizer = None # Fetch the tokenizer for that checkpoint\n","\n","\n","def tokenize_function(example):\n","    # We are using the BERT tokenizer, specifying to PAD until the end,\n","    # truncate if either 128 elements are met or the maximum from the model, which you get from the model card\n","\n","    return pass. # Return a tokenizer function that adds padding to 128 chars and truncates from the examples\n","\n","\n","tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n"],"metadata":{"id":"zZ0eyz_bswnn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's see how it worked!"],"metadata":{"id":"oSt_kIFKvKwv"}},{"cell_type":"code","source":["tokenized_datasets['train'][0]['text']"],"metadata":{"id":"oM3_SxEHaYpg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer(tokenized_datasets['train'][0]['text'])"],"metadata":{"id":"tu6DT9ARvqeL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The tokenizer from BERT (well DistillBERT) converts each word into its ID according to *its* vocabulary. And notice the masking says we haven't been truncated. What we will do know is do this for all data and convert it into a TF Datasets object (which Keras accepts)"],"metadata":{"id":"db-I6E5EvymQ"}},{"cell_type":"code","source":["\n","tf_train_dataset = None # Convert the tokenized_datasets[\"train\"] to a TF Dataset\n","\n","\n","tf_validation_dataset = None. # Same with validation"],"metadata":{"id":"_BkOcOV3YM-i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for inputs, labels in tf_train_dataset.take(1):\n","  print(f' inputs: {inputs.shape}, labels: {labels.shape}')\n"],"metadata":{"id":"fm-TbT_LwMaK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Downloading the model and prepare for training"],"metadata":{"id":"-AKKIC9AzDWP"}},{"cell_type":"markdown","source":["Now let's download the model. It is very important you use the class that starts with `TFAutoModel`. There are auto models for most tasks, so you don't have to manually add the header, for example the `TFAutoModelForSequenceClassification` adds a Dense layer (WITHOUT SOFTMAX) to do the classification"],"metadata":{"id":"B5NK3y0Kvy9n"}},{"cell_type":"code","source":["\n","model = None # Download the model for sequence classification with 2 labels (sentiment analysis)"],"metadata":{"id":"UQ1RBW49vzi2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.optimizers.schedules import PolynomialDecay\n","\n","batch_size = BATCH_SIZE\n","num_epochs = EPOCHS\n","# The number of training steps is the number of samples in the dataset, divided by the batch size then multiplied\n","# by the total number of epochs. Note that the tf_train_dataset here is a batched tf.data.Dataset,\n","# not the original Hugging Face Dataset, so its len() is already num_samples // batch_size.\n","num_train_steps = len(tf_train_dataset) * num_epochs\n","lr_scheduler = PolynomialDecay(\n","    initial_learning_rate=5e-5, end_learning_rate=1e-8, decay_steps=num_train_steps\n",")\n","from tensorflow.keras.optimizers import Adam\n","\n","opt = Adam(learning_rate=lr_scheduler)"],"metadata":{"id":"QpdCdwVVc7av"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loss = None  # Set the loss\n","# Compile the model"],"metadata":{"id":"v7wxa25Yc7l2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["early_stopping = tf.keras.callbacks.EarlyStopping(patience=PATIENCE)\n"],"metadata":{"id":"RsRSw2GufIfh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.summary()"],"metadata":{"id":"5_4l0JM6e7tx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Oh no! We have too many parameters to train! Luckily in Keras is very easy to set some layers as not trainable"],"metadata":{"id":"w0IY68FyxQ5Q"}},{"cell_type":"code","source":["# Set the first layer as non trainable"],"metadata":{"id":"2z7l4zzOglb5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.summary()"],"metadata":{"id":"w-7KUG_qgxLA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["*Voilá!*"],"metadata":{"id":"2XwbQq6_xXjJ"}},{"cell_type":"code","source":["# Fit the model"],"metadata":{"id":"a5NEOuZkdJl2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we have a trained model that did transfer learning from DistillBERT"],"metadata":{"id":"bh7F4SHfiXjA"}},{"cell_type":"markdown","source":["## Testing it out!"],"metadata":{"id":"Ai85jC1HzGSI"}},{"cell_type":"code","source":["tokens = tokenizer([\"This is the worst internet service provider\", \"Although most people say this is the worst, I like it\"], padding=True, truncation=True, max_length=128)"],"metadata":{"id":"G4Tw4BGWdPbu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokens"],"metadata":{"id":"seclxXEgivkR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.predict(tokens['input_ids'])"],"metadata":{"id":"MFfLSI28i2IN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Notice the prediction where not probabilities but logits!"],"metadata":{"id":"dptMVX4Yyz8B"}},{"cell_type":"code","source":["tf.math.softmax(model.predict(tokens['input_ids'])['logits'])"],"metadata":{"id":"h_aQ5cWxkBqM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tf.math.argmax(tf.math.softmax(model.predict(tokens['input_ids'])['logits']))"],"metadata":{"id":"g07INFlujKAG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["And the model was correct!!"],"metadata":{"id":"xehvfI6Qy56b"}},{"cell_type":"code","source":["model.evaluate(tf_validation_dataset)"],"metadata":{"id":"vTev4jIXjwRL"},"execution_count":null,"outputs":[]}]}