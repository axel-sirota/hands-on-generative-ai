{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "# Continuous Bag of Words (CBOW)\n",
    "\n",
    "Â© Data Trainers LLC. GPL v 3.0.\n",
    "\n",
    "**Author:** Axel Sirota\n",
    "\n",
    "\n",
    "In this notebook we will train from scratch a CBOW word embedding model based on a famous dataset: The Yelp reviews dataset. This dataset is uploaded into a dropbox and the cell command to download the files is already done for you.\n",
    "\n",
    "Take it easy and pay attention to the model, how easy it is to define it,and the iteration nuances on the dataset generation.\n",
    "\n",
    "You can run this lab both locally or in Colab.\n",
    "\n",
    "- To run in Colab just go to `https://colab.research.google.com`, sign-in and you upload this notebook. Colab has GPU access for free.\n",
    "- To run locally just run `jupyter notebook` and access the notebook in this lab. You would need to first install the requirements in `requirements.txt`\n",
    "\n",
    "Follow the instructions. Good luck!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_jbFcxFZhG5K"
   },
   "outputs": [],
   "source": [
    "!pip install textblob 'keras-nlp' 'keras-preprocessing' 'gensim==4.2.0' np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iklSJ4lqUQlT"
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Lambda\n",
    "import np_utils\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from textblob import TextBlob, Word\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import warnings\n",
    "import nltk\n",
    "\n",
    "TRACE = False  # Setting to true is useful when debugging to know which device is being used\n",
    "embedding_dim = 50\n",
    "epochs=100\n",
    "batch_size = 500\n",
    "BATCH = True\n",
    "\n",
    "def set_seeds_and_trace():\n",
    "  os.environ['PYTHONHASHSEED'] = '0'\n",
    "  np.random.seed(42)\n",
    "  tf.random.set_seed(42)\n",
    "  random.seed(42)\n",
    "  if TRACE:\n",
    "    tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "def set_session_with_gpus_and_cores():\n",
    "  cores = multiprocessing.cpu_count()\n",
    "  gpus = len(tf.config.list_physical_devices('GPU'))\n",
    "  config = tf.compat.v1.ConfigProto( device_count = {'GPU': gpus  , 'CPU': cores} , intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "  sess = tf.compat.v1.Session(config=config)\n",
    "  tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "set_seeds_and_trace()\n",
    "set_session_with_gpus_and_cores()\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('punkt')\n",
    "textblob_tokenizer = lambda x: TextBlob(x).words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l13de14sclyD"
   },
   "outputs": [],
   "source": [
    "%%writefile get_data.sh\n",
    "if [ ! -f yelp.csv ]; then\n",
    "  wget -O yelp.csv https://www.dropbox.com/s/xds4lua69b7okw8/yelp.csv?dl=0\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PvRXU9EMVJMp"
   },
   "outputs": [],
   "source": [
    "!bash get_data.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QAWXcLEieD4E"
   },
   "outputs": [],
   "source": [
    "path = './yelp.csv'\n",
    "yelp = pd.read_csv(path)\n",
    "# Create a new DataFrame that only contains the 5-star and 1-star reviews.\n",
    "yelp_best_worst = yelp[(yelp.stars==5) | (yelp.stars==1)]\n",
    "X = yelp_best_worst.text\n",
    "y = yelp_best_worst.stars.map({1:0, 5:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ljgSnKkzeM4-"
   },
   "outputs": [],
   "source": [
    "# Create corpus of sentences such that the sentence has more than 3 words\n",
    "corpus = [None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-AyyCRQ2-7J",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "At this point we have a list (any iterable will do) of queries that are longer than 3 words. This is normal to filter random queries. Now we must use the `Tokenizer` object to `fit` on the corpus, in order to convert each wor to an ID, and later convert such corpus of list of words into their identifiers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dUlTe1xsgi51"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "# Use the fit_on_texts method to fit the tokenizer\n",
    "None # Fill\n",
    "\n",
    "print(f'Before the tokenizer: {corpus[:1]}')\n",
    "\n",
    "#Now use the same \"trained\" tokenizer to convert the corpus from words to IDs with the texts_to_sequences method\n",
    "tokenized_corpus = None\n",
    "\n",
    "print(f'After the tokenizer: {tokenized_corpus[:1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ucoEJtOa2-7K",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nb_samples = sum(len(s) for s in tokenized_corpus)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bfR6qIZZhIHd"
   },
   "outputs": [],
   "source": [
    "print(f'First 5 corpus items are {tokenized_corpus[:5]}')\n",
    "print(f'Length of corpus is {len(tokenized_corpus)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2B_z5Udki-_s"
   },
   "outputs": [],
   "source": [
    "type(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B_Z1eJZrhK7K"
   },
   "outputs": [],
   "source": [
    "# This is the algorithmic part of batching the dataset and yielding the window of words and expected middle word for each bacth as a generator.\n",
    "def generate_data(corpus, vocab_size, window_size=2, sentence_batch_size=15,  batch_size=250):\n",
    "    np.random.shuffle(np.array(corpus))\n",
    "    number_of_sentence_batches = (len(corpus) // sentence_batch_size) + 1\n",
    "    for batch in range(number_of_sentence_batches):\n",
    "        lower_end = batch*batch_size\n",
    "        upper_end = (batch+1)*batch_size if batch+1 < number_of_sentence_batches else len(corpus)\n",
    "        mini_batch_size = upper_end - lower_end\n",
    "        maxlen = window_size*2\n",
    "        X = []\n",
    "        Y = []\n",
    "        for review_id, words in enumerate(corpus[lower_end:upper_end]):\n",
    "            L = len(words)\n",
    "            for index, word in enumerate(words):\n",
    "                contexts = []\n",
    "                labels   = []\n",
    "                s = index - window_size\n",
    "                e = index + window_size + 1\n",
    "\n",
    "                contexts.append([words[i] for i in range(s, e) if 0 <= i < L and i != index])\n",
    "                labels.append(word)\n",
    "\n",
    "                x = pad_sequences(contexts, maxlen=maxlen)\n",
    "                y = to_categorical(labels, vocab_size)\n",
    "                X.append(x)\n",
    "                Y.append(y)\n",
    "        X = tf.constant(X)\n",
    "        Y = tf.constant(Y)\n",
    "        number_of_batches = len(X) // batch_size\n",
    "        for real_batch in range(number_of_batches):\n",
    "          lower_end = batch*batch_size\n",
    "          upper_end = (batch+1)*batch_size\n",
    "          batch_X = tf.squeeze(X[lower_end:upper_end])\n",
    "          batch_Y = tf.squeeze(Y[lower_end:upper_end])\n",
    "          yield (batch_X, batch_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "nfsYbRRS2-7N",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Notice now in a sample how we construct X and y to predict words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OvOclN8T2-7N",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "iterable = generate_data(corpus=tokenized_corpus, vocab_size=vocab_size, batch_size=10)\n",
    "sample_x, sample_y = next(iterable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ak0gTIQs2-7O",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sample_y_numpy = sample_y.numpy()\n",
    "\n",
    "sample_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ix8s4Knh2-7O",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "np.where(sample_y_numpy == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QpvnqGOI2-7O",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now comes the core part, defining the model. Keras provides a convenient Sequential model class to just `add` layers of any type and they will just work. Let's add an `Embedding` layer (that will map the word ids into a vector of size 100), a `Lambda` to average the words out in a sentence, and a `Dense layer` to select the best word on the other end. This is classic CBOW.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CHtu75Kpi6XF"
   },
   "outputs": [],
   "source": [
    "cbow = Sequential()\n",
    "cbow.add()  # Add an Embedding layer with input_dim vocab_size, output_dim to be embedding_dim, and the input_length to be twice our window\n",
    "cbow.add()  # Add a Lambda that takes a lambda function using the K.mean method to average the words. The output_shape should be (dim, ).\n",
    "cbow.add()  # Add a classic Dense layer to just select with a softmax the best word\n",
    "# Compile the model with a loss and optimizer of your liking.\n",
    "cbow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yt2xo_G02-7O",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cbow.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g44ICdUcj7ZL"
   },
   "outputs": [],
   "source": [
    "def fit_model():\n",
    "    if not BATCH:\n",
    "        # If we are not batching, Fill how to get X AND Y\n",
    "        X, Y = None # Fill\n",
    "        print(f'Size of X is {X.shape} and Y is {Y.shape}')\n",
    "        cbow.fit(X, Y, epochs = epochs)\n",
    "    else:\n",
    "        # Implement the batching logic to train the model (Hint: use the train_on_batch method of Keras models)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qTM2wqbzke5n"
   },
   "outputs": [],
   "source": [
    "fit_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GR97HVOqkoMI"
   },
   "outputs": [],
   "source": [
    "with open('./cbow_scratch_synonims.txt' ,'w') as f:\n",
    "    f.write('{} {}\\n'.format(vocab_size-1, embedding_dim))\n",
    "    vectors = cbow.get_weights()[0]\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        str_vec = ' '.join(map(str, list(vectors[i, :])))\n",
    "        f.write('{} {}\\n'.format(word, str_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SvMp9eWsk2Z-"
   },
   "outputs": [],
   "source": [
    "w2v = gensim.models.KeyedVectors.load_word2vec_format('./cbow_scratch_synonims.txt', binary=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J0dw_S7Kk6lW"
   },
   "outputs": [],
   "source": [
    "w2v.most_similar(positive=['gasoline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JCmSyCj8k6He"
   },
   "outputs": [],
   "source": [
    "w2v.most_similar(negative=['apple'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wjvrCdY5lkJk"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuClass": "premium",
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}