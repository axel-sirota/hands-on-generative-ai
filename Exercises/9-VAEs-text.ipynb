{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "0HBZa5HtkKUe",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1700418666469,
     "user_tz": 180,
     "elapsed": 575,
     "user": {
      "displayName": "Axel Sirota",
      "userId": "02089179879199828401"
     }
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import ELU, Bidirectional, Dense, Embedding, Input, Lambda, LSTM, RepeatVector, TimeDistributed, Layer, Activation, Dropout\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from scipy import spatial\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import codecs\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "7rpWno_9p-25",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1700418667078,
     "user_tz": 180,
     "elapsed": 611,
     "user": {
      "displayName": "Axel Sirota",
      "userId": "02089179879199828401"
     }
    }
   },
   "execution_count": 62,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8BWTjKtkKUf"
   },
   "source": [
    "### Directories and text loading\n",
    "Initially we will set the main directories and some variables regarding the characteristics of our texts.\n",
    "We set the maximum sequence length to 15, the maximun number of words in our vocabulary to 12000 and we will use 50-dimensional embeddings. Finally we load our texts from a csv. The text file is the train file of the Quora Kaggle challenge containing around 808000 sentences."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "%%writefile get_data.sh\n",
    "if [ ! -f quora.csv ]; then\n",
    "  wget -O quora.csv https://www.dropbox.com/scl/fi/wxvgvw6y48whtuvcx1quq/questions.csv?rlkey=03yokqc36sht66me4jgzmbu12&dl=0\n",
    "fi\n",
    "\n",
    "if [ ! -f glove.6B.100d.txt ]; then\n",
    "  wget -O glove.6B.100d.txt https://www.dropbox.com/s/dl1vswq2sz5f1ws/glove.6B.100d.txt?dl=0\n",
    "fi"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bovnwb2fnFKN",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1700418667078,
     "user_tz": 180,
     "elapsed": 7,
     "user": {
      "displayName": "Axel Sirota",
      "userId": "02089179879199828401"
     }
    },
    "outputId": "3eaac229-2fa0-4b38-aa50-1dbe943f4f71"
   },
   "execution_count": 63,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting get_data.sh\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!bash get_data.sh"
   ],
   "metadata": {
    "id": "_hflxDcrnUrX",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1700418667078,
     "user_tz": 180,
     "elapsed": 4,
     "user": {
      "displayName": "Axel Sirota",
      "userId": "02089179879199828401"
     }
    }
   },
   "execution_count": 64,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CMP_8GBwkKUh",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1700418670088,
     "user_tz": 180,
     "elapsed": 3013,
     "user": {
      "displayName": "Axel Sirota",
      "userId": "02089179879199828401"
     }
    },
    "outputId": "8bf8f0d9-c650-414f-cc65-dcb00474a5aa"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 808702 texts in train.csv\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DATA_FILE = './quora.csv'\n",
    "GLOVE_EMBEDDING = './glove.6B.100d.txt'\n",
    "VALIDATION_SPLIT = 0.2\n",
    "MAX_SEQUENCE_LENGTH = 15\n",
    "MAX_NB_WORDS = 12000\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "texts = []\n",
    "with codecs.open(TRAIN_DATA_FILE, encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    header = next(reader)\n",
    "    for values in reader:\n",
    "        texts.append(values[3])\n",
    "        texts.append(values[4])\n",
    "print('Found %s texts in train.csv' % len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "texts[3]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "pwaMsM6ungYu",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1700418670089,
     "user_tz": 180,
     "elapsed": 17,
     "user": {
      "displayName": "Axel Sirota",
      "userId": "02089179879199828401"
     }
    },
    "outputId": "437eddaa-52bb-451f-b56d-8114a2a8fb30"
   },
   "execution_count": 66,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'What would happen if the Indian government stole the Kohinoor (Koh-i-Noor) diamond back?'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 66
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Embedding"
   ],
   "metadata": {
    "id": "x3-gKWU_noY2"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "path_to_glove_file = \"./glove.6B.100d.txt\"\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4I2cAMkSnp_K",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1700418677129,
     "user_tz": 180,
     "elapsed": 7052,
     "user": {
      "displayName": "Axel Sirota",
      "userId": "02089179879199828401"
     }
    },
    "outputId": "0dbbf4bf-d46d-49a9-accf-1296e62c019c"
   },
   "execution_count": 67,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 400001 word vectors.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gcyp5zjEkKUi"
   },
   "source": [
    "### Text Preprocessing\n",
    "To preprocess the text we will use the tokenizer and the text_to_sequences function from Keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0_FBKILpkKUi",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1700418701032,
     "user_tz": 180,
     "elapsed": 23907,
     "user": {
      "displayName": "Axel Sirota",
      "userId": "02089179879199828401"
     }
    },
    "outputId": "a401ec5a-451a-4ce5-d78c-c87027dcbba5"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 95603 unique tokens\n",
      "Shape of data tensor: (808702, 15)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "word_index = tokenizer.word_index\n",
    "index2word = {v: k for k, v in word_index.items()}\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "data_1 = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', data_1.shape)\n",
    "NB_WORDS = (min(tokenizer.num_words, len(word_index)) + 1 ) #+1 for zero padding\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy\n",
    "numpy.random.shuffle(data_1)\n",
    "training, test = data_1[:int(len(data_1)*VALIDATION_SPLIT)], data_1[int(len(data_1)*VALIDATION_SPLIT):]"
   ],
   "metadata": {
    "id": "0enY6ytHoRFs",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1700418701677,
     "user_tz": 180,
     "elapsed": 654,
     "user": {
      "displayName": "Axel Sirota",
      "userId": "02089179879199828401"
     }
    }
   },
   "execution_count": 69,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ezdic1tbkKUj"
   },
   "source": [
    "### Sentence generator\n",
    "In order to reduce the memory requirements we will gradually read our sentences from the csv through Pandas as we feed them to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "rVbDOrZ7kKUk",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1700418701678,
     "user_tz": 180,
     "elapsed": 26,
     "user": {
      "displayName": "Axel Sirota",
      "userId": "02089179879199828401"
     }
    }
   },
   "outputs": [],
   "source": [
    "def sent_generator(chunksize):\n",
    "    reader = pd.read_csv(TRAIN_DATA_FILE, chunksize=chunksize, iterator=True)\n",
    "    for df in reader:\n",
    "        val3 = df.iloc[:,3:4].values.tolist()\n",
    "        val4 = df.iloc[:,4:5].values.tolist()\n",
    "        flat3 = [item for sublist in val3 for item in sublist]\n",
    "        flat4 = [str(item) for sublist in val4 for item in sublist]\n",
    "        texts = []\n",
    "        texts.extend(flat3[:])\n",
    "        texts.extend(flat4[:])\n",
    "\n",
    "        sequences = tokenizer.texts_to_sequences(texts)\n",
    "        data_train = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "        yield (data_train, data_train)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "next(sent_generator(50))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lu7LtxkBsVKQ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1700418701679,
     "user_tz": 180,
     "elapsed": 27,
     "user": {
      "displayName": "Axel Sirota",
      "userId": "02089179879199828401"
     }
    },
    "outputId": "cdbf806e-e8c1-479c-b9a5-0f4ce53b3e23"
   },
   "execution_count": 71,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([[   0,    2,    3, ...,  383,    8,   35],\n",
       "        [   0,    0,    0, ...,   10,    5, 4565],\n",
       "        [   0,    4,   13, ...,  146,    6, 2773],\n",
       "        ...,\n",
       "        [   0,    0,    0, ...,   22,    1,  140],\n",
       "        [   0,    0,    0, ...,   33, 6892,  730],\n",
       "        [   0,    0,    0, ...,    7,   52,  283]], dtype=int32),\n",
       " array([[   0,    2,    3, ...,  383,    8,   35],\n",
       "        [   0,    0,    0, ...,   10,    5, 4565],\n",
       "        [   0,    4,   13, ...,  146,    6, 2773],\n",
       "        ...,\n",
       "        [   0,    0,    0, ...,   22,    1,  140],\n",
       "        [   0,    0,    0, ...,   33, 6892,  730],\n",
       "        [   0,    0,    0, ...,    7,   52,  283]], dtype=int32))"
      ]
     },
     "metadata": {},
     "execution_count": 71
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwD7HBn7kKUl"
   },
   "source": [
    "### Word embeddings\n",
    "We will use pretrained Glove word embeddings as embeddings for our network. We create a matrix with one embedding for every word in our vocabulary and then we will pass this matrix as weights to the keras embedding layer of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G8W-SqXWkKUl",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1700418701679,
     "user_tz": 180,
     "elapsed": 21,
     "user": {
      "displayName": "Axel Sirota",
      "userId": "02089179879199828401"
     }
    },
    "outputId": "2219ef21-c066-4324-a0d0-2a742aacc6f4"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Null word embeddings: 1\n"
     ]
    }
   ],
   "source": [
    "glove_embedding_matrix = np.zeros((NB_WORDS, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i < NB_WORDS:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be the word embedding of 'unk'.\n",
    "            glove_embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            glove_embedding_matrix[i] = embeddings_index.get('unk')\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(glove_embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sp5H3ZILkKUm"
   },
   "source": [
    "### VAE model\n",
    "Our model is based on a seq2seq architecture with a bidirectional LSTM encoder and an LSTM decoder and SELU activations.\n",
    "We feed the latent representation at every timestep as input to the decoder through \"RepeatVector(max_len)\".\n",
    "\n",
    "We use the sum of the BCE loss on the final sentences generated + the KL loss from the Sampling layer.\n",
    "\n",
    "Moreover, due to the pandas iterator that reads the csv both the train size and validation size must be divisible by the batch_size."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "batch_size = 100\n",
    "max_len = MAX_SEQUENCE_LENGTH\n",
    "emb_dim = EMBEDDING_DIM\n",
    "latent_dim = 32\n",
    "intermediate_dim = 96\n",
    "epsilon_std = 1.0\n",
    "num_sampled=500"
   ],
   "metadata": {
    "id": "wMXN-N8DRGwN",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1700418701680,
     "user_tz": 180,
     "elapsed": 18,
     "user": {
      "displayName": "Axel Sirota",
      "userId": "02089179879199828401"
     }
    }
   },
   "execution_count": 73,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class Sampling(tf.keras.layers.Layer):\n",
    "  def call(self, inputs):\n",
    "    \"\"\"Generates a random sample and combines with the encoder output\n",
    "\n",
    "    Args:\n",
    "      inputs -- output tensor from the encoder\n",
    "\n",
    "    Returns:\n",
    "      `inputs` tensors combined with a random sample\n",
    "    \"\"\"\n",
    "\n",
    "    # implement\n",
    "    pass"
   ],
   "metadata": {
    "id": "_BBL36GsQNVM",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1700418701680,
     "user_tz": 180,
     "elapsed": 17,
     "user": {
      "displayName": "Axel Sirota",
      "userId": "02089179879199828401"
     }
    }
   },
   "execution_count": 74,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from keras.initializers import Constant\n",
    "\n",
    "def encoder_layers(inputs, latent_dim):\n",
    "  \"\"\"Defines the encoder's layers.\n",
    "  Args:\n",
    "    inputs -- batch from the dataset\n",
    "    latent_dim -- dimensionality of the latent space\n",
    "\n",
    "  Returns:\n",
    "    mu -- learned mean\n",
    "    sigma -- learned standard deviation\n",
    "    batch_2.shape -- shape of the features before flattening\n",
    "  \"\"\"\n",
    "\n",
    "  x_embed = Embedding(NB_WORDS, emb_dim, embeddings_initializer=Constant(glove_embedding_matrix), input_length=max_len, trainable=False, name='embedding')(inputs)\n",
    "  h = Bidirectional(LSTM(intermediate_dim, return_sequences=False, recurrent_dropout=0.2), merge_mode='concat', name='bidirectional_lstm_1')(x_embed)\n",
    "  h = Dropout(0.2)(h)\n",
    "  h = Dense(intermediate_dim, activation='relu')(h)\n",
    "  h = Dropout(0.2)(h)\n",
    "  mu = Dense(latent_dim, name='latent_mu')(h)\n",
    "  sigma = Dense(latent_dim, name='latent_sigma')(h)\n",
    "  return mu, sigma, intermediate_dim"
   ],
   "metadata": {
    "id": "txuWVmNpQTxS",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1700418701680,
     "user_tz": 180,
     "elapsed": 15,
     "user": {
      "displayName": "Axel Sirota",
      "userId": "02089179879199828401"
     }
    }
   },
   "execution_count": 75,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def encoder_model(latent_dim, input_shape):\n",
    "  \"\"\"Defines the encoder model with the Sampling layer\n",
    "  Args:\n",
    "    latent_dim -- dimensionality of the latent space\n",
    "    input_shape -- shape of the dataset batch\n",
    "\n",
    "  Returns:\n",
    "    model -- the encoder model\n",
    "    conv_shape -- shape of the features before flattening\n",
    "  \"\"\"\n",
    "\n",
    "  # declare the inputs tensor with the given shape\n",
    "  inputs = None\n",
    "\n",
    "  # get the output of the encoder_layers() function\n",
    "  mu, sigma, shape = None\n",
    "\n",
    "  # feed mu and sigma to the Sampling layer\n",
    "  z = None\n",
    "\n",
    "  # build the whole encoder model\n",
    "  model = None\n",
    "\n",
    "  return model, shape"
   ],
   "metadata": {
    "id": "9Mhim_RYREo_",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1700418701680,
     "user_tz": 180,
     "elapsed": 14,
     "user": {
      "displayName": "Axel Sirota",
      "userId": "02089179879199828401"
     }
    }
   },
   "execution_count": 76,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def decoder_layers(inputs, shape):\n",
    "  \"\"\"Defines the decoder layers.\n",
    "  Args:\n",
    "    inputs -- output of the encoder\n",
    "    shape -- shape of the features before flattening\n",
    "\n",
    "  Returns:\n",
    "    tensor containing the decoded output\n",
    "  \"\"\"\n",
    "\n",
    "  x = RepeatVector(max_len)(inputs)\n",
    "  x = LSTM(intermediate_dim, return_sequences=True, recurrent_dropout=0.2)(x)\n",
    "  x = TimeDistributed(Dense(NB_WORDS, activation='softmax'))(x)\n",
    "  return x"
   ],
   "metadata": {
    "id": "X7Hdr08PRdBQ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1700418701680,
     "user_tz": 180,
     "elapsed": 13,
     "user": {
      "displayName": "Axel Sirota",
      "userId": "02089179879199828401"
     }
    }
   },
   "execution_count": 77,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def decoder_model(latent_dim, shape):\n",
    "  \"\"\"Defines the decoder model.\n",
    "  Args:\n",
    "    latent_dim -- dimensionality of the latent space\n",
    "    shape -- shape of the features before flattening\n",
    "\n",
    "  Returns:\n",
    "    model -- the decoder model\n",
    "  \"\"\"\n",
    "\n",
    "  # set the inputs to the shape of the latent space\n",
    "  inputs = None\n",
    "\n",
    "  # get the output of the decoder layers\n",
    "  outputs = None\n",
    "\n",
    "  # declare the inputs and outputs of the model\n",
    "  model = None\n",
    "\n",
    "  return model"
   ],
   "metadata": {
    "id": "GLaSgRJLSMb_",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1700418701681,
     "user_tz": 180,
     "elapsed": 14,
     "user": {
      "displayName": "Axel Sirota",
      "userId": "02089179879199828401"
     }
    }
   },
   "execution_count": 78,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def kl_reconstruction_loss(mu, sigma):\n",
    "  \"\"\" Computes the Kullback-Leibler Divergence (KLD)\n",
    "  Args:\n",
    "    mu -- mean\n",
    "    sigma -- standard deviation\n",
    "\n",
    "  Returns:\n",
    "    KLD loss\n",
    "  \"\"\"\n",
    "  kl_loss = 1 + sigma - tf.square(mu) - tf.math.exp(sigma)\n",
    "  kl_loss = tf.reduce_mean(kl_loss) * -0.5\n",
    "\n",
    "  return kl_loss"
   ],
   "metadata": {
    "id": "g_6Z7DpvSU4J",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1700418701681,
     "user_tz": 180,
     "elapsed": 13,
     "user": {
      "displayName": "Axel Sirota",
      "userId": "02089179879199828401"
     }
    }
   },
   "execution_count": 79,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def vae_model(encoder, decoder, input_shape):\n",
    "  \"\"\"Defines the VAE model\n",
    "  Args:\n",
    "    encoder -- the encoder model\n",
    "    decoder -- the decoder model\n",
    "    input_shape -- shape of the dataset batch\n",
    "\n",
    "  Returns:\n",
    "    the complete VAE model\n",
    "  \"\"\"\n",
    "\n",
    "  # set the inputs\n",
    "  inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "\n",
    "  # get mu, sigma, and z from the encoder output\n",
    "  mu, sigma, z = encoder(inputs)\n",
    "\n",
    "  # get reconstructed output from the decoder\n",
    "  reconstructed = decoder(z)\n",
    "\n",
    "  # define the inputs and outputs of the VAE\n",
    "  model = tf.keras.Model(inputs=inputs, outputs=reconstructed)\n",
    "\n",
    "  # add the KL loss\n",
    "  kl_loss = kl_reconstruction_loss(mu, sigma)\n",
    "  model.add_loss(kl_loss)\n",
    "\n",
    "  return model, kl_loss"
   ],
   "metadata": {
    "id": "4AbhTK0KSYn1",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1700418701681,
     "user_tz": 180,
     "elapsed": 13,
     "user": {
      "displayName": "Axel Sirota",
      "userId": "02089179879199828401"
     }
    }
   },
   "execution_count": 80,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def get_models(input_shape, latent_dim):\n",
    "  \"\"\"Returns the encoder, decoder, and vae models\"\"\"\n",
    "  encoder, shape = encoder_model(latent_dim=latent_dim, input_shape=input_shape)\n",
    "  decoder = decoder_model(latent_dim=latent_dim, shape=shape)\n",
    "  vae, kl_loss = vae_model(encoder, decoder, input_shape=input_shape)\n",
    "  return encoder, decoder, vae, kl_loss"
   ],
   "metadata": {
    "id": "KK_2u821Sea3",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1700418701682,
     "user_tz": 180,
     "elapsed": 13,
     "user": {
      "displayName": "Axel Sirota",
      "userId": "02089179879199828401"
     }
    }
   },
   "execution_count": 81,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "encoder, decoder, vae, kl_loss = get_models(input_shape=(max_len), latent_dim=latent_dim)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m7P47KcASkiK",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1700418702182,
     "user_tz": 180,
     "elapsed": 513,
     "user": {
      "displayName": "Axel Sirota",
      "userId": "02089179879199828401"
     }
    },
    "outputId": "37803944-499e-4a01-b6bb-8e16b4ea437c"
   },
   "execution_count": 82,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "def custom_loss(y_true, y_pred):\n",
    "    print(y_true)\n",
    "    print(y_pred)\n",
    "    flattened_inputs = tf.cast(tf.reshape(y_true, shape=[-1]), dtype=tf.float32)\n",
    "    flattened_outputs = tf.cast(tf.reshape(tf.math.argmax(y_pred, axis=2), shape=[-1]), dtype=tf.float32)\n",
    "    bce_loss = tf.keras.losses.BinaryCrossentropy()(flattened_inputs, flattened_outputs) * max_len * batch_size\n",
    "    total_loss = bce_loss + kl_loss\n",
    "    return total_loss"
   ],
   "metadata": {
    "id": "dgN4iLKZTlYE",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1700418702182,
     "user_tz": 180,
     "elapsed": 6,
     "user": {
      "displayName": "Axel Sirota",
      "userId": "02089179879199828401"
     }
    }
   },
   "execution_count": 83,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "vae.compile(optimizer=optimizer, loss=custom_loss)\n"
   ],
   "metadata": {
    "id": "DzxHhagMagLf",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1700418702707,
     "user_tz": 180,
     "elapsed": 25,
     "user": {
      "displayName": "Axel Sirota",
      "userId": "02089179879199828401"
     }
    }
   },
   "execution_count": 84,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "vae.summary()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IstJ6Xj3UAjt",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1700418702707,
     "user_tz": 180,
     "elapsed": 24,
     "user": {
      "displayName": "Axel Sirota",
      "userId": "02089179879199828401"
     }
    },
    "outputId": "47cf93f2-d91a-4e04-bcaf-fac9c506b0f0"
   },
   "execution_count": 85,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_6 (InputLayer)        [(None, 15)]                 0         []                            \n",
      "                                                                                                  \n",
      " model_3 (Functional)        [(None, 32),                 1376132   ['input_6[0][0]']             \n",
      "                              (None, 32),                                                         \n",
      "                              (None, 32)]                                                         \n",
      "                                                                                                  \n",
      " model_4 (Functional)        (None, 15, 12001)            1213633   ['model_3[0][2]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TF  (None, 32)                   0         ['model_3[0][1]']             \n",
      " OpLambda)                                                                                        \n",
      "                                                                                                  \n",
      " tf.math.square_1 (TFOpLamb  (None, 32)                   0         ['model_3[0][0]']             \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.subtract_2 (TFOpLa  (None, 32)                   0         ['tf.__operators__.add_1[0][0]\n",
      " mbda)                                                              ',                            \n",
      "                                                                     'tf.math.square_1[0][0]']    \n",
      "                                                                                                  \n",
      " tf.math.exp_1 (TFOpLambda)  (None, 32)                   0         ['model_3[0][1]']             \n",
      "                                                                                                  \n",
      " tf.math.subtract_3 (TFOpLa  (None, 32)                   0         ['tf.math.subtract_2[0][0]',  \n",
      " mbda)                                                               'tf.math.exp_1[0][0]']       \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean_1 (TFO  ()                           0         ['tf.math.subtract_3[0][0]']  \n",
      " pLambda)                                                                                         \n",
      "                                                                                                  \n",
      " tf.math.multiply_1 (TFOpLa  ()                           0         ['tf.math.reduce_mean_1[0][0]'\n",
      " mbda)                                                              ]                             \n",
      "                                                                                                  \n",
      " add_loss_1 (AddLoss)        ()                           0         ['tf.math.multiply_1[0][0]']  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2589765 (9.88 MB)\n",
      "Trainable params: 1389665 (5.30 MB)\n",
      "Non-trainable params: 1200100 (4.58 MB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# # Training loop.\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  print('Start of epoch %d' % (epoch + 1,))\n",
    "\n",
    "  # iterate over the batches of the dataset.\n",
    "  for step, (x_train, x_train) in enumerate(sent_generator(batch_size/2)):\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "      # feed a batch to the VAE model\n",
    "      reconstructed = vae(tf.constant(x_train))\n",
    "      # add KLD regularization loss\n",
    "      loss = vae.losses\n",
    "\n",
    "    # get the gradients and update the weights\n",
    "    grads = tape.gradient(loss, vae.trainable_weights)\n",
    "    optimizer.apply_gradients(\n",
    "    (grad, var)\n",
    "    for (grad, var) in zip(grads, vae.trainable_variables)\n",
    "    if grad is not None\n",
    "    )\n",
    "    loss_metric = tf.keras.metrics.Mean()\n",
    "    # compute the loss metric\n",
    "    loss_metric(loss)\n",
    "\n",
    "    # display outputs every 100 steps\n",
    "    if step % 100 == 0:\n",
    "      print('Epoch: %s step: %s mean loss = %s' % (epoch + 1, step, loss_metric.result().numpy()))\n",
    "    if step % 1000 == 0 and step != 0:\n",
    "      break"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q1tRyWZsTm0O",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1700419766735,
     "user_tz": 180,
     "elapsed": 965274,
     "user": {
      "displayName": "Axel Sirota",
      "userId": "02089179879199828401"
     }
    },
    "outputId": "54be491f-a9df-45a1-a638-71ed282c4178"
   },
   "execution_count": 88,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Start of epoch 0\n",
      "Epoch: 1 step: 0 mean loss = 0.0041612466\n",
      "Epoch: 1 step: 100 mean loss = 2.8871e-08\n",
      "Epoch: 1 step: 200 mean loss = 5.525723e-07\n",
      "Epoch: 1 step: 300 mean loss = 1.6713328e-06\n",
      "Epoch: 1 step: 400 mean loss = 1.0272488e-08\n",
      "Epoch: 1 step: 500 mean loss = 3.7252903e-09\n",
      "Epoch: 1 step: 600 mean loss = 9.450596e-07\n",
      "Epoch: 1 step: 700 mean loss = 2.2117048e-07\n",
      "Epoch: 1 step: 800 mean loss = 9.313226e-10\n",
      "Epoch: 1 step: 900 mean loss = 2.7939677e-09\n",
      "Epoch: 1 step: 1000 mean loss = 2.7939677e-09\n",
      "Start of epoch 1\n",
      "Epoch: 2 step: 0 mean loss = 9.313226e-09\n",
      "Epoch: 2 step: 100 mean loss = 7.450581e-09\n",
      "Epoch: 2 step: 200 mean loss = 3.7252903e-09\n",
      "Epoch: 2 step: 300 mean loss = 5.5879354e-09\n",
      "Epoch: 2 step: 400 mean loss = 9.313226e-10\n",
      "Epoch: 2 step: 500 mean loss = 5.5879354e-09\n",
      "Epoch: 2 step: 600 mean loss = 1.540035e-07\n",
      "Epoch: 2 step: 700 mean loss = 1.0775402e-08\n",
      "Epoch: 2 step: 800 mean loss = -9.313226e-10\n",
      "Epoch: 2 step: 900 mean loss = 9.313226e-09\n",
      "Epoch: 2 step: 1000 mean loss = 4.656613e-09\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "vfhd3AKUjuci",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1700418704647,
     "user_tz": 180,
     "elapsed": 6,
     "user": {
      "displayName": "Axel Sirota",
      "userId": "02089179879199828401"
     }
    }
   },
   "execution_count": 86,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GP5AYpaUkKUo"
   },
   "source": [
    "### Project and sample sentences from the latent space\n",
    "Now we build an encoder model model that takes a sentence and projects it on the latent space and a decoder model that goes from the latent space back to the text representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SeGHwhePkKUo"
   },
   "source": [
    "### Test on validation sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Nm0_561kKUo",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1700419778974,
     "user_tz": 180,
     "elapsed": 12253,
     "user": {
      "displayName": "Axel Sirota",
      "userId": "02089179879199828401"
     }
    },
    "outputId": "36212bfe-90df-4cca-8afe-d92f3fb3c5a2"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "625/625 [==============================] - 6s 9ms/step\n",
      "313/313 [==============================] - 2s 5ms/step\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[None,\n",
       " 'is',\n",
       " 'ios',\n",
       " 'supposed',\n",
       " 'to',\n",
       " 'clear',\n",
       " 'out',\n",
       " 'your',\n",
       " 'music',\n",
       " 'library',\n",
       " 'when',\n",
       " 'you',\n",
       " 'cross',\n",
       " 'national',\n",
       " 'borders']"
      ]
     },
     "metadata": {},
     "execution_count": 89
    }
   ],
   "source": [
    "index2word = {v: k for k, v in word_index.items()}\n",
    "sent_encoded = encoder.predict(test[:10000], batch_size = 16)\n",
    "x_test_reconstructed = decoder.predict(sent_encoded[0])\n",
    "\n",
    "sent_idx = 672\n",
    "reconstructed_indexes = np.apply_along_axis(np.argmax, 1, x_test_reconstructed[sent_idx])\n",
    "#np.apply_along_axis(np.max, 1, x_test_reconstructed[sent_idx])\n",
    "#np.max(np.apply_along_axis(np.max, 1, x_test_reconstructed[sent_idx]))\n",
    "word_list = list(np.vectorize(index2word.get)(reconstructed_indexes))\n",
    "word_list\n",
    "original_sent = list(np.vectorize(index2word.get)(test[sent_idx]))\n",
    "original_sent"
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "haITKNDN6UBd"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "A100"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}