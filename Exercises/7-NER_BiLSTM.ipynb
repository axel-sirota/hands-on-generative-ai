{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "# Bidirectional LSTM for Named Entity Recognition\n",
    "\n",
    "© Data Trainers LLC. GPL v 3.0.\n",
    "\n",
    "**Author:** Axel Sirota\n",
    "\n",
    "\n",
    "In this notebook we will train a Bi-LSTM on NER dataset to create the appropriate tags\n",
    "\n",
    "Take it easy and notice the structure of the dataset to ensure for each word if it has a tag or not\n",
    "\n",
    "You can run this lab both locally or in Colab.\n",
    "\n",
    "- To run in Colab just go to `https://colab.research.google.com`, sign-in and you upload this notebook. Colab has GPU access for free.\n",
    "- To run locally just run `jupyter notebook` and access the notebook in this lab. You would need to first install the requirements in `requirements.txt`\n",
    "\n",
    "Follow the instructions. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FO4He6AuvCvD",
    "outputId": "ca0b128e-d8ba-4a2c-b9a9-a140916fe290",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695606448699,
     "user_tz": 180,
     "elapsed": 11,
     "user": {
      "displayName": "Axel Sirota",
      "userId": "02089179879199828401"
     }
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/bin/bash: line 1: nvidia-smi: command not found\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qT84LO_ryWGC",
    "outputId": "df85a4bc-b7c6-4f32-d51d-06d9ef013144",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695606472437,
     "user_tz": 180,
     "elapsed": 23742,
     "user": {
      "displayName": "Axel Sirota",
      "userId": "02089179879199828401"
     }
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
      "Collecting keras-nlp\n",
      "  Downloading keras_nlp-0.6.2-py3-none-any.whl (590 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m590.1/590.1 kB\u001B[0m \u001B[31m7.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting keras-preprocessing\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m42.6/42.6 kB\u001B[0m \u001B[31m5.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting gensim==4.2.0\n",
      "  Downloading gensim-4.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (24.0 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m24.0/24.0 MB\u001B[0m \u001B[31m56.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting np_utils\n",
      "  Downloading np_utils-0.6.0.tar.gz (61 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m62.0/62.0 kB\u001B[0m \u001B[31m7.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Collecting swifter\n",
      "  Downloading swifter-1.4.0.tar.gz (1.2 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.2/1.2 MB\u001B[0m \u001B[31m70.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from gensim==4.2.0) (1.23.5)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.10/dist-packages (from gensim==4.2.0) (1.11.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim==4.2.0) (6.4.0)\n",
      "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)\n",
      "Collecting keras-core (from keras-nlp)\n",
      "  Downloading keras_core-0.1.7-py3-none-any.whl (950 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m950.8/950.8 kB\u001B[0m \u001B[31m58.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras-nlp) (1.4.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-nlp) (23.1)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from keras-nlp) (2023.6.3)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras-nlp) (13.5.2)\n",
      "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from keras-nlp) (0.1.8)\n",
      "Collecting tensorflow-text (from keras-nlp)\n",
      "  Downloading tensorflow_text-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.5/6.5 MB\u001B[0m \u001B[31m90.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from keras-preprocessing) (1.16.0)\n",
      "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from swifter) (1.5.3)\n",
      "Requirement already satisfied: psutil>=5.6.6 in /usr/local/lib/python3.10/dist-packages (from swifter) (5.9.5)\n",
      "Requirement already satisfied: dask[dataframe]>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from swifter) (2023.8.1)\n",
      "Requirement already satisfied: tqdm>=4.33.0 in /usr/local/lib/python3.10/dist-packages (from swifter) (4.66.1)\n",
      "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]>=2.10.0->swifter) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]>=2.10.0->swifter) (2.2.1)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]>=2.10.0->swifter) (2023.6.0)\n",
      "Requirement already satisfied: partd>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]>=2.10.0->swifter) (1.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]>=2.10.0->swifter) (6.0.1)\n",
      "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]>=2.10.0->swifter) (0.12.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]>=2.10.0->swifter) (6.8.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->swifter) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->swifter) (2023.3.post1)\n",
      "Collecting namex (from keras-core->keras-nlp)\n",
      "  Downloading namex-0.0.7-py3-none-any.whl (5.8 kB)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras-core->keras-nlp) (3.9.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras-nlp) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras-nlp) (2.16.1)\n",
      "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text->keras-nlp) (0.14.0)\n",
      "Requirement already satisfied: tensorflow<2.14,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text->keras-nlp) (2.13.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.13.0->dask[dataframe]>=2.10.0->swifter) (3.16.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras-nlp) (0.1.2)\n",
      "Requirement already satisfied: locket in /usr/local/lib/python3.10/dist-packages (from partd>=1.2.0->dask[dataframe]>=2.10.0->swifter) (1.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (23.5.26)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (1.57.0)\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (2.13.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (16.0.6)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (67.7.2)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (2.13.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (4.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (1.15.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (0.33.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (0.41.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (2.17.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (3.4.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (2.3.7)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->keras-nlp) (3.2.2)\n",
      "Building wheels for collected packages: np_utils, swifter\n",
      "  Building wheel for np_utils (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for np_utils: filename=np_utils-0.6.0-py3-none-any.whl size=56438 sha256=301861d3bf1ee4079b550e37b403df57ee7e4cef179d707278de6efb5ebf9b1b\n",
      "  Stored in directory: /root/.cache/pip/wheels/b6/c7/50/2307607f44366dd021209f660045f8d51cb976514d30be7cc7\n",
      "  Building wheel for swifter (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for swifter: filename=swifter-1.4.0-py3-none-any.whl size=16507 sha256=9ed318752173a2a154e998523cc243bbdc60996497e18fdb43beeba147cd910e\n",
      "  Stored in directory: /root/.cache/pip/wheels/e4/cf/51/0904952972ee2c7aa3709437065278dc534ec1b8d2ad41b443\n",
      "Successfully built np_utils swifter\n",
      "Installing collected packages: namex, np_utils, keras-preprocessing, gensim, keras-core, swifter, tensorflow-text, keras-nlp\n",
      "  Attempting uninstall: gensim\n",
      "    Found existing installation: gensim 4.3.2\n",
      "    Uninstalling gensim-4.3.2:\n",
      "      Successfully uninstalled gensim-4.3.2\n",
      "Successfully installed gensim-4.2.0 keras-core-0.1.7 keras-nlp-0.6.2 keras-preprocessing-1.1.2 namex-0.0.7 np_utils-0.6.0 swifter-1.4.0 tensorflow-text-2.13.0\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob 'keras-nlp' 'keras-preprocessing' 'gensim==4.2.0' np_utils swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6h9-CflyyWIf",
    "outputId": "83d554ee-6983-46b9-a3b9-21bafd429a4d",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695606486878,
     "user_tz": 180,
     "elapsed": 14454,
     "user": {
      "displayName": "Axel Sirota",
      "userId": "02089179879199828401"
     }
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using TensorFlow backend\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras import Model, Input\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense, Embedding, Lambda, ELU, Conv1D, MaxPooling1D, Dropout, LSTM, TimeDistributed, SpatialDropout1D, Bidirectional\n",
    "import np_utils\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from textblob import TextBlob, Word\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.initializers import Constant\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "import keras_nlp\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import warnings\n",
    "import nltk\n",
    "import pickle\n",
    "from tensorflow.nn import leaky_relu\n",
    "\n",
    "import re\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from textblob import TextBlob\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "TRACE = False\n",
    "embedding_dim = 100\n",
    "rnn_units = 128\n",
    "epochs=100\n",
    "buffer_size = 256\n",
    "max_len = 50\n",
    "# Batch size\n",
    "batch_size = 256\n",
    "min_count_words = 3\n",
    "BATCH = True\n",
    "\n",
    "def set_seeds_and_trace():\n",
    "  os.environ['PYTHONHASHSEED'] = '0'\n",
    "  np.random.seed(42)\n",
    "  tf.random.set_seed(42)\n",
    "  random.seed(42)\n",
    "  if TRACE:\n",
    "    tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "def set_session_with_gpus_and_cores():\n",
    "  cores = multiprocessing.cpu_count()\n",
    "  gpus = len(tf.config.list_physical_devices('GPU'))\n",
    "  config = tf.compat.v1.ConfigProto( device_count = {'GPU': gpus  , 'CPU': cores} , intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "  sess = tf.compat.v1.Session(config=config)\n",
    "  tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "set_seeds_and_trace()\n",
    "set_session_with_gpus_and_cores()\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('punkt')\n",
    "textblob_tokenizer = lambda x: TextBlob(x).words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0oENn5_JyWLG",
    "outputId": "da37478f-85cd-45b4-ba4d-bf4fd4a562c5",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695606486880,
     "user_tz": 180,
     "elapsed": 22,
     "user": {
      "displayName": "Axel Sirota",
      "userId": "02089179879199828401"
     }
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Writing get_data.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile get_data.sh\n",
    "if [ ! -f ner_dataset.csv ]; then\n",
    "  wget -O ner_dataset.csv https://www.dropbox.com/s/mbfv0x988mdj89h/ner_dataset.csv?dl=0\n",
    "fi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "CkxCOJz_yWNu",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695606489046,
     "user_tz": 180,
     "elapsed": 2181,
     "user": {
      "displayName": "Axel Sirota",
      "userId": "02089179879199828401"
     }
    },
    "outputId": "ea3037f4-3117-4a1b-e2cc-92eb9764fbb4"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--2023-09-25 01:48:06--  https://www.dropbox.com/s/mbfv0x988mdj89h/ner_dataset.csv?dl=0\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.8.18, 2620:100:601b:18::a27d:812\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.8.18|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: /s/raw/mbfv0x988mdj89h/ner_dataset.csv [following]\n",
      "--2023-09-25 01:48:07--  https://www.dropbox.com/s/raw/mbfv0x988mdj89h/ner_dataset.csv\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://uc7d1f2c8eea5c1790b0d68f4c57.dl.dropboxusercontent.com/cd/0/inline/CEWLE2o04dKgVHdXTr9F6DK-G-BY7iDetf-rtonDKDQQ0s1xfzmpc8jzWewTsnypcpy0XTpNufH-HN-ikIm1b7YTd94cJY5NBbRHozFhRydrBC7WODSFCeirhportY8xgqVSPK9j8hlqe7fRerMKNfJ4/file# [following]\n",
      "--2023-09-25 01:48:07--  https://uc7d1f2c8eea5c1790b0d68f4c57.dl.dropboxusercontent.com/cd/0/inline/CEWLE2o04dKgVHdXTr9F6DK-G-BY7iDetf-rtonDKDQQ0s1xfzmpc8jzWewTsnypcpy0XTpNufH-HN-ikIm1b7YTd94cJY5NBbRHozFhRydrBC7WODSFCeirhportY8xgqVSPK9j8hlqe7fRerMKNfJ4/file\n",
      "Resolving uc7d1f2c8eea5c1790b0d68f4c57.dl.dropboxusercontent.com (uc7d1f2c8eea5c1790b0d68f4c57.dl.dropboxusercontent.com)... 162.125.8.15, 2620:100:601b:15::a27d:80f\n",
      "Connecting to uc7d1f2c8eea5c1790b0d68f4c57.dl.dropboxusercontent.com (uc7d1f2c8eea5c1790b0d68f4c57.dl.dropboxusercontent.com)|162.125.8.15|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 15208151 (15M) [text/plain]\n",
      "Saving to: ‘ner_dataset.csv’\n",
      "\n",
      "ner_dataset.csv     100%[===================>]  14.50M  36.2MB/s    in 0.4s    \n",
      "\n",
      "2023-09-25 01:48:08 (36.2 MB/s) - ‘ner_dataset.csv’ saved [15208151/15208151]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!bash get_data.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18V5m92oyWQK"
   },
   "outputs": [],
   "source": [
    "data= pd.read_csv(\"./ner_dataset.csv\",encoding=\"latin1\")\n",
    "data = None # Forward Fill NaN\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8RzRPJPTyWS2"
   },
   "outputs": [],
   "source": [
    "print(\"Unique Words in corpus:\",data['Word'].nunique())\n",
    "print(\"Unique Tag in corpus:\",data['Tag'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oYF9u8Dp13YW"
   },
   "outputs": [],
   "source": [
    "words = None # Get list of words\n",
    "words.append(\"ENDPAD\")\n",
    "num_words = len(words)\n",
    "tags = None # Get list of tags\n",
    "num_tags = len(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1XAv4P3G1-7D"
   },
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "  def __init__(self,data):\n",
    "    self.n_sent = 1 #counter\n",
    "    self.data = data\n",
    "    agg_func = lambda s:[(w,p,t) for w,p,t in zip(s['Word'].tolist(),s['POS'].tolist(),s['Tag'].tolist())]\n",
    "    self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
    "    self.sentences = [s for s in self.grouped]\n",
    "\n",
    "\n",
    "\n",
    "getter = SentenceGetter(data)\n",
    "sentences = getter.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9tP7TzZ71-9d"
   },
   "outputs": [],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R4IPXmOQ1_Ci"
   },
   "outputs": [],
   "source": [
    "word2idx =  {w : i+1 for i,w in enumerate(words)}\n",
    "tag2idx  =  {t : i for i,t in enumerate(tags)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ox9i1Zc1_FN"
   },
   "outputs": [],
   "source": [
    "X = None # Construct tensor of X with the ids of words and pad to the right\n",
    "\n",
    "y = None # Construct tensor of y with the ids of tags and pad to the right\n",
    "y = [to_categorical(i, num_classes=num_tags) for i in y]\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y, test_size=0.1, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z1JGVp1x30AY"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape = (max_len,)))\n",
    "model.add() # Add embedding layer\n",
    "model.add()  # Add SpatialDropout\n",
    "model.add()  # Add a Bi-LSTM with 100 units and a recurrent dropout for forgetfulness\n",
    "model.add(TimeDistributed(Dense(num_tags,activation='softmax')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "877hAFmB4lJK"
   },
   "outputs": [],
   "source": [
    "# Compile the model, use  kullback leibler divergence as metric (we will talk why)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D3-ZFBRX4soC"
   },
   "outputs": [],
   "source": [
    "# Train the model with early stopping if the validation kullback leibler metrics goes up, and return the best weights!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QF6I4FYU5LKf"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DFgXTGOE5Mia"
   },
   "outputs": [],
   "source": [
    "i = np.random.randint(0, x_test.shape[0])\n",
    "p = model.predict(np.array([x_test[i]]))\n",
    "p = np.argmax(p, axis=-1)\n",
    "\n",
    "y_true = np.argmax(np.array(y_test), axis=-1)[i]\n",
    "print(\"{:15}{:5}\\t {}\\n\".format(\"Word\",\"True\",\"Pred\"))\n",
    "print(\"-\"*30)\n",
    "for w,true,pred in zip(x_test[i], y_true, p[0]):\n",
    "  print(\"{:15}{:5}\\t{}\".format(words[w-1], tags[true],tags[pred]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DCyQIGqG6alm"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}