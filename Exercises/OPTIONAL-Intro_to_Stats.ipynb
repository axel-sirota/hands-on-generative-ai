{"cells":[{"cell_type":"markdown","metadata":{"id":"jv5kVSdXED31"},"source":["# Intro to Statistics\n","\n","© Data Trainers LLC. GPL v 3.0.\n","\n","Author: Axel Sirota\n","\n"]},{"cell_type":"markdown","metadata":{"id":"rg00M533ED33"},"source":["<a id=\"learning-objectives\"></a>\n","## Learning Objectives\n","- Compute dot products, matrix multiplications, and vector norms by hand and using NumPy.\n","- Code summary statistics using NumPy and Pandas: mean, median, mode, max, min, quartile, inter-quartile range, variance, standard deviation, and correlation.\n","- Create basic data visualizations, including scatterplots, box plots, and histograms.\n","- Describe characteristics and trends in a data set using visualizations.\n","- Describe the bias and variance of statistical estimators.\n","- Identify a normal distribution within a data set using summary statistics and data visualizations."]},{"cell_type":"markdown","metadata":{"id":"QQC9zTGDED34"},"source":["### Lesson Guide\n","- [Where Are We in the Data Science Workflow?](#where-are-we-in-the-data-science-workflow)\n","- [Linear Algebra Review](#linear-algebra-review)\n","\n","- [Linear Algebra Applications to Machine Learning](#linear-algebra-applications-to-machine-learning)\n","- [Code-Along: Examining the Titanic Data Set](#codealong-examining-the-titanic-dataset)\n","- [Descriptive Statistics Fundamentals](#descriptive-statistics-fundamentals)\n","- [Our First Model](#our-first-model)\n","- [A Short Introduction to Model Bias and Variance](#a-short-introduction-to-model-bias-and-variance)\n","- [Correlation and Association](#correlation-and-association)\n","- [The Normal Distribution](#the-normal-distribution)\n","- [Determining the Distribution of Your Data](#determining-the-distribution-of-your-data)\n","- [Lesson Review](#topic-review)"]},{"cell_type":"markdown","metadata":{"id":"mcjZxrQsED35"},"source":["<a id=\"where-are-we-in-the-data-science-workflow\"></a>\n","## Where Are We in the Data Science Workflow?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WV9YQVCQED36"},"outputs":[],"source":["### install ipywidgets\n","!pip install ipywidgets numpy pandas scipy matplotlib ipython scikit-learn\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PJWfMWb4ED36"},"outputs":[],"source":["!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TJ60yWG1ED36"},"outputs":[],"source":["import pandas as pd\n","import seaborn as sns\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# https://ipywidgets.readthedocs.io/en/latest/examples/Using%20Interact.html\n","from ipywidgets import interact\n","\n","import scipy.stats\n","\n","plt.style.use('fivethirtyeight')\n","\n","# This makes sure that graphs render in your notebook.\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"Q5qtPaIJED37"},"source":["<a id=\"linear-algebra-review\"></a>\n","## Linear Algebra Intro/Review\n","---\n","**Objective:** Compute dot products, matrix multiplications, and vector norms by hand and using NumPy."]},{"cell_type":"markdown","metadata":{"id":"L6T2ArkgED38"},"source":["<a id=\"why-linear-algebra\"></a>\n","## Why Use Linear Algebra in Data Science?\n","\n","Linear models are efficient and well understood. They can often closely approximate nonlinear solutions, and they scale to high dimensions without difficulty.\n","\n","Because of these desirable properties, linear algebra is a need-to-know subject for machine learning. In fact, it forms the basis of foundational models such as linear regression, logistic regression, and principal component analysis (PCA).\n","\n","Unsurprisingly, advanced models such as neural networks and support vector machines rely on linear algebra as their \"trick\" for impressive speedups. Modern-day GPUs are essentially linear algebra supercomputers. And, to utilize their power on a GPU, models must often be carefully formulated in terms of vectors and matrices.\n","\n","More than that, today's advanced models build upon the simpler foundational models. Each neuron in a neural net is essentially a logistic regressor! Support vector machines utilize a kernel trick to craftily make problems linear that would not otherwise appear to be.\n","\n","Although we do not have time in this course to comprehensively discuss linear algebra, we highly recommend you become fluent!"]},{"cell_type":"markdown","metadata":{"id":"8iwomymhED38"},"source":["<a id=\"scalars-vectors-and-matrices\"></a>\n","### Scalars, Vectors, and Matrices\n","\n","A **scalar** is a single number. Here, symbols that are lowercase single letters refer to scalars. For example, the symbols $a$ and $v$ are scalars that might refer to arbitrary numbers such as $5.328$ or $7$. An example scalar would be:\n","\n","$$a$$\n","\n","A **vector** is an ordered sequence of numbers. Here, symbols that are lowercase single letters with an arrow — such as $\\vec{u}$ — refer to vectors. An example vector would be:\n","\n","$$\\vec{u} = \\left[ \\begin{array}{c}\n","1&3&7\n","\\end{array} \\right]$$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"venQBnIgED39"},"outputs":[],"source":["# Create a vector using np.array.\n","u = np.array([1, 3, 7])\n","u"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zLk1ZVE3ED39"},"outputs":[],"source":["u.shape"]},{"cell_type":"markdown","metadata":{"id":"2g5fKUUrED39"},"source":["An $m$ x $n$ **matrix** is a rectangular array of numbers with $m$ rows and $n$ columns. Each number in the matrix is an entry. Entries can be denoted $a_{ij}$, where $i$ denotes the row number and $j$ denotes the column number. Note that, because each entry $a_{ij}$ is a lowercase single letter, a matrix is an array of scalars:\n","\n","$$\\mathbf{A}= \\left[ \\begin{array}{c}\n","a_{11} & a_{12} & ... & a_{1n}  \\\\\n","a_{21} & a_{22} & ... & a_{2n}  \\\\\n","... & ... & ... & ... \\\\\n","a_{m1} & a_{m2} & ... & a_{mn}\n","\\end{array} \\right]$$\n","\n","Matrices are referred to using bold uppercase letters, such as $\\mathbf{A}$. A bold font face is used to distinguish matrices from sets."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OwHtC3zvED4F"},"outputs":[],"source":["# Create a matrix using np.array.\n","m = np.array([[1, 3, 7], [4, 6, 3], [2, 5, 6]])\n","m"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9PN6_J2gED4G"},"outputs":[],"source":["m.shape"]},{"cell_type":"markdown","metadata":{"id":"yhgeMCmHED4G"},"source":["> Note: in Python, a matrix is just a list of lists! The outermost list is a list of rows."]},{"cell_type":"markdown","metadata":{"id":"hhqytpbqED4G"},"source":["## How adding a dimension can help"]},{"cell_type":"markdown","metadata":{"id":"HN2U-gKUED4G"},"source":["<img src=\"https://www.dropbox.com/scl/fi/fiofmkqv9jy8e4ha3770z/svm.png?rlkey=nnkrc2c5s9wb48tyrw6ktbjym&raw=1\"  align=\"center\"/>"]},{"cell_type":"markdown","metadata":{"id":"o0mjKKESED4G"},"source":["<a id=\"basic-matrix-algebra\"></a>\n","## Basic Matrix Algebra\n"]},{"cell_type":"markdown","metadata":{"id":"ctvyQ2AUED4G"},"source":["### Addition and Subtraction\n","Vector **addition** is straightforward. If two vectors are of equal dimensions (The vectors are shown here as column vectors for convenience only):\n","\n","$\\vec{v} = \\left[ \\begin{array}{c}\n","1 \\\\\n","3 \\\\\n","7\n","\\end{array} \\right],  \\vec{w} = \\left[ \\begin{array}{c}\n","1 \\\\\n","0 \\\\\n","1\n","\\end{array} \\right]$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e2EVKecNED4G"},"outputs":[],"source":["v = np.array([1, 3, 7])\n","w = np.array([1, 0, 1])"]},{"cell_type":"markdown","metadata":{"id":"ER8SI6M-ED4H"},"source":["$\\vec{v} + \\vec{w} =\n","\\left[ \\begin{array}{c}\n","1 \\\\\n","3 \\\\\n","7\n","\\end{array} \\right] + \\left[ \\begin{array}{c}\n","1 \\\\\n","0 \\\\\n","1\n","\\end{array} \\right] =\n","\\left[ \\begin{array}{c}\n","1+1 \\\\\n","3+0 \\\\\n","7+1\n","\\end{array} \\right] =\n","\\left[ \\begin{array}{c}\n","2 \\\\\n","3 \\\\\n","8\n","\\end{array} \\right]\n","$\n","\n","(Subtraction is similar.)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3nstFwJeED4H"},"outputs":[],"source":["# Add the vectors together with +.\n","v + w"]},{"cell_type":"markdown","metadata":{"id":"Xtry2RGBED4H"},"source":["### Scalar Multiplication\n","We scale a vector with **scalar multiplication**, multiplying a vector by a scalar (single quantity):\n","\n","$ 2 \\cdot \\vec{v} = 2\\left[ \\begin{array}{c}\n","1 \\\\\n","3 \\\\\n","7\n","\\end{array} \\right] =\n"," \\left[ \\begin{array}{c}\n","2 \\cdot 1 \\\\\n","2 \\cdot 3 \\\\\n","2 \\cdot 7\n","\\end{array} \\right] =\n"," \\left[ \\begin{array}{c}\n","2 \\\\\n","6 \\\\\n","14\n","\\end{array} \\right]$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iB621HZBED4H"},"outputs":[],"source":["# Multiply v by 2.\n","2 * np.array([1, 3, 7])"]},{"cell_type":"markdown","metadata":{"id":"XoyNkTQnED4I"},"source":["<a id=\"dot-product\"></a>\n","## Dot Product\n","\n","Measure of similarity\n","\n","\n","The **dot product** of two _n_-dimensional vectors is:\n","\n","$ \\vec{v} \\cdot \\vec{w} =\\sum _{i=1}^{n}v_{i}w_{i}=v_{1}w_{1}+v_{2}w_{2}+\\cdots +v_{n}w_{n} $\n","\n","So, if:\n","\n","$\\vec{v} = \\left[ \\begin{array}{c}\n","1 \\\\\n","3 \\\\\n","7\n","\\end{array} \\right], \\vec{w} = \\left[ \\begin{array}{c}\n","1 \\\\\n","0 \\\\\n","1\n","\\end{array} \\right]$\n","\n","$ \\vec{v} \\cdot \\vec{w} = 1 \\cdot 1 + 3 \\cdot 0 + 7 \\cdot 1 = 8 $"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k4uaKKZZED4I"},"outputs":[],"source":["v = np.array([1, 3, 7])\n","w = np.array([1, 0, 1])\n","\n","# Calculate the dot product of v and w using np.dot.\n","v.dot(w)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QBfz9l52ED4I"},"outputs":[],"source":["z= np.array([-7, 0, 1])\n","v.dot(z)"]},{"cell_type":"markdown","metadata":{"id":"-PCcKDV0ED4I"},"source":["<a id=\"matrix-multiplication\"></a>\n","### Matrix Multiplication\n","**Matrix multiplication**, $\\mathbf{A}_{mn}$ x $\\mathbf{B}_{ij}$, is valid when the left matrix has the same number of columns as the right matrix has rows ($n = i$). Each entry is the dot product of corresponding row and column vectors.\n","\n","<img src=\"https://www.dropbox.com/scl/fi/faqgkxfc1h8eiv2xgoc59/matrix-multiply-a.gif?rlkey=wxfo91bgd8w38e6rtvtsls5h2&raw=1\"  align=\"center\"/>"]},{"cell_type":"markdown","metadata":{"id":"aKjQQ1bCED4J"},"source":["The dot product illustrated above is: $1 \\cdot 7 + 2 \\cdot 9 + 3 \\cdot 11 = 58$. Can you compute the rest of the dot products by hand?\n","\n","If the product is the $2$ x $2$ matrix $\\mathbf{C}_{mj}$, then:\n","\n","+ Matrix entry $c_{12}$ (its FIRST row and SECOND column) is the dot product of the FIRST row of $\\mathbf{A}$ and the SECOND column of $\\mathbf{B}$.\n","\n","+ Matrix entry $c_{21}$ (its SECOND row and FIRST column) is the dot product of the SECOND row of $\\mathbf{A}$ and the FIRST column of $\\mathbf{B}$.\n","\n","Note that if the first matrix is $m$ x $n$ ($m$ rows and $n$ columns) and the second is  $i$ x $j$ (where $n = i$), then the final matrix will be $m$ x $j$. For example, below we have $2$ x $3$ multiplied by $3$ x $2$, which results in a $2$ x $2$ matrix. Can you see why?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F7RPq1SGED4J"},"outputs":[],"source":["A = np.array([[1, 2, 3], [4, 5, 6]])  # 2x3\n","B = np.array([[7, 8], [9, 10], [11, 12]])  # 3x2\n","print(A)\n","print(B)\n","A.dot(B)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UxDueTAcED4J"},"outputs":[],"source":["B.dot(A)   #  A x B != B x A\n","\n","# there are many A matrices such that there does not exist B matrix with the following property:\n","# B * A = A * B =  1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JcttYxJfED4J"},"outputs":[],"source":["4*7+5*9+6*11"]},{"cell_type":"markdown","metadata":{"id":"Z70bwvJWED4K"},"source":["Make sure you can compute this by hand!"]},{"cell_type":"markdown","metadata":{"id":"0Jm0L4vEED4K"},"source":["<a id=\"n-dimensional-space\"></a>\n","### N-Dimensional Space\n","\n","We often refer to vectors as elements of an $n$-dimensional space. The symbol $\\mathbb{R}$ refers to the set of all real numbers (written in uppercase \"blackboard bold\" font). Because this contains all reals, $3$ and $\\pi$ are **contained in** $\\mathbb{R}$. We often write this symbolically as $3 \\in \\mathbb{R}$ and $\\pi \\in \\mathbb{R}$.\n","\n","To get the set of all pairs of real numbers, we would essentially take the product of this set with itself (called the Cartesian product) — $\\mathbb{R}$ x $\\mathbb{R}$, abbreviated as $\\mathbb{R}^2$. This set — $\\mathbb{R}^2$ — contains all pairs of real numbers, so $(1, 3)$ is **contained in** this set. We write this symbolically as $(1, 3) \\in \\mathbb{R}^2$.\n","\n","+ In 2-D space ($\\mathbb{R}^2$), a point is uniquely referred to using two coordinates: $(1, 3) \\in \\mathbb{R}^2$.\n","+ In 3-D space ($\\mathbb{R}^3$), a point is uniquely referred to using three coordinates: $(8, 2, -3) \\in \\mathbb{R}^3$.\n","+ In $n$-dimensional space ($\\mathbb{R}^n$), a point is uniquely referred to using $n$ coordinates.\n","\n","Note that these coordinates of course are isomorphic to our vectors! After all, coordinates are ordered sequences of numbers, just as we define vectors to be ordered sequences of numbers. So, especially in machine learning, we often visualize vectors of length $n$ as points in $n$-dimensional space."]},{"cell_type":"markdown","metadata":{"id":"wUR8J-KRED4K"},"source":["<a id=\"vector-norm\"></a>\n","### Vector Norm\n","\n","The **magnitude** of a vector, $\\vec{v} \\in \\mathbb{R}^{n}$, can be interpreted as its length in $n$-dimensional space. Therefore it is calculable via the Euclidean distance from the origin:\n","\n","$\\vec{v} = \\left[ \\begin{array}{c}\n","v_{1} \\\\\n","v_{2} \\\\\n","\\vdots \\\\\n","v_{n}\n","\\end{array} \\right]$\n","\n","then $\\| \\vec{v} \\| = \\sqrt{v_{1}^{2} + v_{2}^{2} + ... + v_{n}^{2}} = \\sqrt{v^Tv}$\n","\n","E.g. if $\\vec{v} =\n","\\left[ \\begin{array}{c}\n","3 \\\\\n","4\n","\\end{array} \\right]$, then $\\| \\vec{v} \\| = \\sqrt{3^{2} + 4^{2}} = 5$\n","\n","This is also called the vector **norm**. You will often see this used in machine learning."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KwU6qiAxED4K"},"outputs":[],"source":["x = np.array([3,4])"]},{"cell_type":"markdown","metadata":{"id":"tQHvkSTVED4K"},"source":["# Calculate the norm of the vector x with np.linalg.norm.\n","\n","<img src=\"https://www.dropbox.com/scl/fi/r0b7cvcfioqbk4985hzye/hands_on.jpg?rlkey=jukxyfvobw2s86ti2qxj71vid&raw=1\" width=\"100\" height=\"100\" align=\"right\"/>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pc0AeGIkED4L"},"outputs":[],"source":["## Write your code here\n"]},{"cell_type":"markdown","metadata":{"id":"dib1wKq3ED4L"},"source":["### A visual Example\n","<img src=\"https://www.dropbox.com/scl/fi/bbgs6ea3bxmddof11z07x/lr.png?rlkey=7xjf4ytdo49tgndszl48wmme5&raw=1\"  align=\"center\"/>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X4LbwfjqED4L"},"outputs":[],"source":["# X-> matrix of size nxm  (n = rows=datapoints, m=features/columns)\n","# y -> of size nx1 (n \"real values\")\n","\n","# y_pred = X*W + b   where W needs to be m x 1, b is a scalar\n","\n","# price =  2*sq_foot + 3*bedrroms - 5 * roof_type = [2,3,5] * X"]},{"cell_type":"markdown","metadata":{"id":"nS_58F5lED4L"},"source":["<a id=\"linear-algebra-applications-to-machine-learning\"></a>\n","## Linear Algebra Applications to Machine Learning\n","---\n","\n","<a id=\"distance-between-actual-values-and-predicted-values\"></a>\n","### Distance Between Actual Values and Predicted Values\n","We often need to know the difference between predicted values and actual values. In 2-D space, we compute this as:\n","$$\\| \\vec{actual} - \\vec{predicted} \\| =\\sqrt{(actual_1 - predicted_1)^2 + (actual_2 - predicted_2)^2}$$\n","\n","Note that this is just the straight-line distance between the actual point and the predicted point.\n","\n","<a id=\"mean-squared-error\"></a>\n","### Mean Squared Error\n","Often, it's easier to look at the mean of the squared errors. Where $\\hat{y}(\\mathbf{X})$ is a vector of predicted values (a function of the data matrix $\\mathbf{X}$) and $\\vec{y}$ is the actual values:\n","\n","$$MSE = \\frac{1} {n} \\| \\hat{y}(\\mathbf{X}) - \\vec{y} \\|^2$$\n","\n","<a id=\"least-squares\"></a>\n","### Least squares\n","Many machine learning models are based on the following form:\n","\n","$$\\min \\| \\hat{y}(\\mathbf{X}) - \\vec{y} \\|$$\n","\n","The goal is to minimize the distance between model predictions and actual data."]},{"cell_type":"markdown","metadata":{"id":"8WSfexfIED4L"},"source":["<a id=\"our-first-model\"></a>\n","# Our First Model\n","---\n","\n","In this section, we will make a **mathematical model** of data. When we say **model**, we mean it in the same sense that a toy car is a **model** of a real car. If we mainly care about appearance, the toy car model is an excellent model. However, the toy car fails to accurately represent other aspects of the car. For example, we cannot use a toy car to test how the actual car would perform in a collision.\n","\n","In data science, we might take a rich, complex person and model that person solely as a two-dimensional vector: _(age, smokes cigarettes)_. For example: $(90, 1)$, $(28, 0)$, and $(52, 1)$, where $1$ indicates \"smokes cigarettes.\" This model of a complex person obviously fails to account for many things. However, if we primarily care about modeling health risk, it might provide valuable insight.\n","\n","Now that we have superficially modeled a complex person, we might determine a formula that evaluates risk. For example, an older person tends to have worse health, as does a person who smokes. So, we might deem someone as having risk should `age + 50*smokes > 100`.\n","\n","This is a **mathematical model**, as we use math to assess risk. It could be mostly accurate. However, there are surely elderly people who smoke who are in excellent health.\n","\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"XFYfsIPGED4M"},"source":["### Get the `fare` column from the Titanic data and store it in variable `y`:"]},{"cell_type":"code","source":["%%writefile get_data.sh\n","mkdir -p data\n","if [ ! -f data/titanic.csv ]; then\n","  wget -O data/titanic.csv https://www.dropbox.com/scl/fi/csnl3vpbq94i4vxpfoe2w/titanic.csv?rlkey=6q576c7lp0e25tb5khvz066l9&dl=0\n","fi"],"metadata":{"id":"ONwm5QGDJJSR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!bash get_data.sh"],"metadata":{"id":"ixtoUv3QJUXR"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4gh-rbIrED4M"},"outputs":[],"source":["import pandas as pd\n","titanic = pd.read_csv('data/titanic.csv')\n","titanic.columns = titanic.columns.str.lower()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s_WOQ27xED4M"},"outputs":[],"source":["titanic.head()"]},{"cell_type":"markdown","metadata":{"id":"oZ2zyTfSED4M"},"source":["<img src=\"https://www.dropbox.com/scl/fi/b4tw9swsqfbv1wcm1vhgc/titanic.png?rlkey=po2nz10rkd7u96srhmedaciex&raw=1\"  align=\"left\"/>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2TVQMRl5ED4M"},"outputs":[],"source":["# Get the fare column from the Titanic data and store it as y:\n","y = titanic.fare\n","y"]},{"cell_type":"markdown","metadata":{"id":"5U4ZqiKxED4N"},"source":["### Create predictions `y_pred` (in this case just the mean of `y`):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dA9a969sED4N"},"outputs":[],"source":["# Stored predictions in y_pred:\n","y_pred = titanic.fare.mean()\n","y_pred"]},{"cell_type":"markdown","metadata":{"id":"Bzv2OMroED4N"},"source":["### Find the average squared distance between each prediction and its actual value:\n","\n","This is known as the mean squared error (MSE)."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"b9cy6UMlED4N"},"outputs":[],"source":["# Squared error is hard to read; let's look at mean squared error\n","diff_squared=(y-y_pred)**2\n","diff_squared_sum=diff_squared.sum()\n","diff_squared_sum_mean=diff_squared_sum/len(y)\n","diff_squared_sum_mean"]},{"cell_type":"markdown","metadata":{"id":"pvbldWm2ED4N"},"source":["#### Calculate the root mean squared error (RMSE), the square root of the MSE:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"47uq4lhvED4P"},"outputs":[],"source":["np.sqrt(diff_squared_sum_mean)"]},{"cell_type":"markdown","metadata":{"id":"GQrw6UdOED4P"},"source":["\n","---\n","\n","# Let's Review the Titanic Dataset\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"sYf1kVtpED4P"},"source":["### Objective: Read in the Titanic data and look at a few summary statistics."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3-fwVIg8ED4Q"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"VVU4p2C3ED4Q"},"source":["### Print out the column names:"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"8UzbqxdrED4Q"},"outputs":[],"source":["# Answer:\n"]},{"cell_type":"markdown","metadata":{"id":"tBvUQsAYED4Q"},"source":["### Print out the dimensions of the DataFrame using the `.shape` attribute:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"npGIBw8_ED4Q"},"outputs":[],"source":["# Preview data dimensions.\n"]},{"cell_type":"markdown","metadata":{"id":"B9FSD7L7ED4R"},"source":["### Print out the data types of the columns using the `.dtypes` attribute:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nAChHKQQED4R"},"outputs":[],"source":["# What are the column data types?\n"]},{"cell_type":"markdown","metadata":{"id":"7XleTspTED4R"},"source":["### Use the built-in  `.value_counts()` function to count the values of each type in the `pclass` column:"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"DinpsTCJED4R"},"outputs":[],"source":["# Count the values of the plcass variable.\n"]},{"cell_type":"markdown","metadata":{"id":"omspkks3ED4R"},"source":["### Generate descriptive statistics for each variable using the built-in `.describe()` function:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EuszVLgWED4R"},"outputs":[],"source":["# Descriptive statistics for each variable.\n"]},{"cell_type":"markdown","metadata":{"id":"J3cz_77bED4S"},"source":["### Diagnosing Data Problems\n","\n","- Whenever you get a new data set, the fastest way to find mistakes and inconsistencies is to look at the descriptive statistics.\n","  - If anything looks too high or too low relative to your experience, there may be issues with the data collection.\n","- Your data may contain a lot of missing values and may need to be cleaned meticulously before they can be combined with other data.\n","  - You can take a quick average or moving average to smooth out the data and combine that to preview your results before you embark on your much longer data-cleaning journey.\n","  - Sometimes filling in missing values with their means or medians will be the best solution for dealing with missing data. Other times, you may want to drop the offending rows or do real imputation."]},{"cell_type":"markdown","metadata":{"id":"FZnGVda4ED4S"},"source":["<a id=\"descriptive-statistics-fundamentals\"></a>\n","## Descriptive Statistics Fundamentals\n","---\n","\n","- **Objective:** Code summary statistics using NumPy and Pandas: mean, median, mode, max, min, quartile, inter-quartile range, variance, standard deviation, and correlation."]},{"cell_type":"markdown","metadata":{"id":"ZyxiJbe3ED4S"},"source":["### A Quick Review of Notation"]},{"cell_type":"markdown","metadata":{"id":"k6BLec4oED4S"},"source":["The sum of a constant, $k$, $n$ times:\n","$$\\sum_{i=1}^nk$$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KabRWGhEED4S"},"outputs":[],"source":["# k + k + k + k + ... + k sum n times = n*k"]},{"cell_type":"markdown","metadata":{"id":"a7-hTHooED4S"},"source":["> It is often helpful to think of these sums as `for` loops. For example, the equation can be compactly computed like so:\n","\n","```\n","total = 0\n","\n","# For i from 1 up to and including n, add k to the sum.\n","for i in range(1, n+1):\n","    total += k\n","```\n","\n","> Or, even more succinctly (using a generator comprehension):\n","\n","```\n","total = sum(k for i in range(1, n+1))\n","```"]},{"cell_type":"markdown","metadata":{"id":"5rll1fVrED4T"},"source":["The sum of all numbers from 1 up to and including $n$:\n","$$\\sum_{i=1}^ni$$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lfJnh4pEED4T"},"outputs":[],"source":["# 1 + 2 + 3 + ... + n = n*(n+1)/2"]},{"cell_type":"markdown","metadata":{"id":"uACms_F-ED4T"},"source":["> ```\n","total = sum(i for i in range(1, n+1))\n","```"]},{"cell_type":"markdown","metadata":{"id":"asmIOPVvED4T"},"source":["The sum of all $x$ from the first $x$ entry to the $n$th $x$ entry:\n","$$\\sum_{i=0}^nx_i$$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DKyYgNWuED4T"},"outputs":[],"source":["# x_0 + x_1 + x_2 + x_3 + ... + x_n"]},{"cell_type":"markdown","metadata":{"id":"pVnLGjDeED4T"},"source":["> ```\n","total = sum(xi in x)      # or just sum(x)\n","```"]},{"cell_type":"markdown","metadata":{"id":"1z9iXSL9ED4T"},"source":["<a id=\"measures-of-central-tendency\"></a>\n","# Measures of Central Tendency"]},{"cell_type":"markdown","metadata":{"id":"0HWR8IF8ED4U"},"source":["- Mean: the average of a dataset\n","- Median: the middle of an ordered dataset; not susceptible to outliers\n","- Mode: the most common value in a dataset; only relevant for discrete data"]},{"cell_type":"markdown","metadata":{"id":"0aahXf6NED4U"},"source":["### Mean\n","The mean — also known as the average or expected value — is defined as:\n","$$E[X] = \\bar{X} =\\frac 1n\\sum_{i=1}^nx_i$$\n","\n","It is determined by summing all data points in a population and then dividing the total by the number of points. The resulting number is known as the mean or the average.\n","\n","Be careful — the mean can be highly affected by outliers. For example, the mean of a very large number and some small numbers will be much larger than the \"typical\" small numbers. Earlier, we saw that the mean squared error (MSE) was used to optimize linear regression. Because this mean is highly affected by outliers, the resulting linear regression model is, too."]},{"cell_type":"markdown","metadata":{"id":"vpb35i8pED4U"},"source":["### Median\n","The median refers to the midpoint in a series of numbers. Notice that the median is not affected by outliers, so it more so represents the \"typical\" value in a set.\n","\n","$$ 0,1,2,[3],5,5,1004 $$\n","\n","$$ 1,3,4,[4,5],5,5,7 $$\n","\n","To find the median:\n","\n","- Arrange the numbers in order from smallest to largest.\n","    - If there is an odd number of values, the middle value is the median.\n","    - If there is an even number of values, the average of the middle two values is the median.\n","\n","Although the median has many useful properties, the mean is easier to use in optimization algorithms. The median is more often used in analysis than in machine learning algorithms."]},{"cell_type":"markdown","metadata":{"id":"MxAkn4PIED4U"},"source":["### Mode\n","The mode of a set of values is the value that occurs most often.\n","A set of values may have more than one mode, or no mode at all.\n","\n","$$1,0,1,5,7,8,9,3,4,1$$\n","\n","$1$ is the mode, as it occurs the most often (three times)."]},{"cell_type":"markdown","metadata":{"id":"_wCWgR3vED4V"},"source":["## How Mean, Median and Mode Demonstrate Skewness\n","\n","<img src=\"https://www.dropbox.com/scl/fi/42l73f9wn7la6kjz7gskq/Skew.jpg?rlkey=ot626jcnv10pweayaslrxi6ph&raw=1\"  align=\"center\"/>"]},{"cell_type":"markdown","metadata":{"id":"ktJysJQ2ED4V"},"source":["## Code-Along\n","<img src=\"https://www.dropbox.com/scl/fi/r0b7cvcfioqbk4985hzye/hands_on.jpg?rlkey=jukxyfvobw2s86ti2qxj71vid&raw=1\" width=\"100\" height=\"100\" align=\"right\"/>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"68SKTlJyED4V"},"outputs":[],"source":["# Find the mean of the titanic.fare series using base Python:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hd3QptR-ED4V"},"outputs":[],"source":["# Find the mean of the titanic.fare series using NumPy\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6JrvikUKED4V"},"outputs":[],"source":["# Find the mean of the titanic.fare series using Pandas:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DDf63BJkED4V"},"outputs":[],"source":["# What was the median fare paid (using Pandas)?\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MHfo34HjED4X"},"outputs":[],"source":["# Use Pandas to find the most common fare paid on the Titanic:\n"]},{"cell_type":"markdown","metadata":{"id":"TvOkI793ED4X"},"source":["# Plot the distribution\n","<img src=\"https://www.dropbox.com/scl/fi/r0b7cvcfioqbk4985hzye/hands_on.jpg?rlkey=jukxyfvobw2s86ti2qxj71vid&raw=1\" width=\"100\" height=\"100\" align=\"right\"/>\n","\n","1. What kind of skew do we have here?\n","2. Plot the right visualization to show it"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YsSsXDCGED4X"},"outputs":[],"source":["## do your plot here"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mJC6ig30ED4X"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ROJsKJWTED4Y"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"xD-9nVPBED4Y"},"source":["## Is the titanic fare distribution symmetrical?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CycdqAOgED4Y"},"outputs":[],"source":["## yes / no , right skewed or left skewed."]},{"cell_type":"markdown","metadata":{"id":"hAQD-nhnED4Y"},"source":["### Calculating Mode using scipy library"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2-Arj1diED4Z"},"outputs":[],"source":["from scipy.stats import mode\n","mode([3, 4, 5, 3, 5, 5, 4, 3, 5, 7, 6])"]},{"cell_type":"markdown","metadata":{"id":"2jyCo_EuED4Z"},"source":["<a id=\"math-review\"></a>\n","## Math Review\n","\n","### How Do We Measure Distance?\n","\n","One method is to take the difference between two points:\n","\n","$$X_2 - X_1$$\n","\n","However, this can be inconvenient because of negative numbers.\n","\n","We often use the following square root trick to deal with negative numbers. Note this is equivalent to the absolute value (if the points are 1-D):\n","\n","$$\\sqrt{(X_2-X_1)^2} = | X_2 - X_1 |$$\n","\n","#### What About Distance in Multiple Dimensions?\n","\n","We can turn to the Pythagorean theorem.\n","\n","$$a^2 + b^2 = c^2$$\n","\n","To find the distance along a diagonal, it is sufficient to measure one dimension at a time:\n","\n","$$\\sqrt{a^2 + b^2} = c$$\n","\n","More generally, we can write this as the norm (You'll see this in machine learning papers):\n","\n","$$\\|X\\|_2 = \\sqrt{\\sum{x_i^2}} = c$$\n","\n","What if we want to work with points rather than distances? For points $\\vec{x}: (x_1, x_1)$ and $\\vec{y}: (y_1, y_2)$ we can write:\n","\n","$$\\sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2} = c$$\n","or\n","$$\\sqrt{\\sum{(x_i - y_i)^2}} = c$$\n","or\n","$$\\| \\vec{x} - \\vec{y} \\| = c$$\n","\n","> You may be more familiar with defining points as $(x, y)$ rather than $(x_1, x_2)$. However, in machine learning it is much more convenient to define each coordinate using the same base letter with a different subscript. This allows us to easily represent a 100-dimensional point, e.g., $(x_1, x_2, ..., x_{100})$. If we use the grade school method, we would soon run out of letters!"]},{"cell_type":"markdown","metadata":{"id":"Q9t2HMv8ED4a"},"source":["<a id=\"measures-of-dispersion-standard-deviation-and-variance\"></a>\n","# Measures of Dispersion\n","\n","> Standard Deviation and Variance"]},{"cell_type":"markdown","metadata":{"id":"JEU7Y1SDED4a"},"source":["## Standard Deviation\n","\n","SD, $σ$ for population standard deviation, or $s$ for sample standard deviation is a measure that is used to quantify the amount of variation or dispersion from the mean of a set of data values. A low standard deviation means that most of the numbers are close to the average. A high standard deviation means that the numbers are spread out.\n","\n","Standard deviation is the square root of variance:\n","\n","$$variance = \\frac {\\sum{(x_i - \\bar{X})^2}} {n-1}$$\n","\n","$$s = \\sqrt{\\frac {\\sum{(x_i - \\bar{X})^2}} {n-1}}$$\n","\n","> **Standard deviation** is often used because it is in the same units as the original data! By glancing at the standard deviation, we can immediately estimate how \"typical\" a data point might be by how many standard deviations it is from the mean. Furthermore, standard deviation is the only value that makes sense to visually draw alongside the original data.\n","\n","> **Variance** is often used for efficiency in computations. The square root in the SD always increases with the function to which it is applied. So, removing it can simplify calculations (e.g., taking derivatives), particularly if we are using the variance for tasks such as optimization."]},{"cell_type":"markdown","metadata":{"id":"HkJF9rf7ED4a"},"source":["## Calculating Variance by Hand"]},{"cell_type":"markdown","metadata":{"id":"oZEmhuueED4b"},"source":["Let's walk through an example of computing the variance by hand.\n","\n","Suppose we have the following data:\n","\n","$$X = [1, 2, 3, 4, 4, 10]$$\n","\n","First, we compute its mean:\n","\n","$$\\bar{X} = (1/6)(1 + 2 + 3 + 4 + 4 + 10) = 4$$\n","\n","Because this is a sample of data rather than the full population, we'll use the second formula. Let's first \"mean center\" the data:\n","\n","$$X_{centered} = X - \\bar{X} = [-3, -2, -1, 0, 0, 6]$$\n","\n","Now, we'll simply find the average squared distance of each point from the mean:\n","\n","$$variance = \\frac {\\sum{(x_i - \\bar{X})^2}} {n-1} = \\frac {(-3)^2 + (-2)^2 + (-1)^2 + 0^2 + 0^2 + 6^2}{6-1} = \\frac{14 + 36}{5} = 10$$\n","\n","So, the **variance of $X$** is $10$. However, we cannot compare this directly to the original units, because it is in the original units squared. So, we will use the **standard deviation of $X$**, $\\sqrt{10} \\approx 3.16$ to see that the value of $10$ is farther than one standard deviation from the mean of $4$. So, we can conclude it is somewhat far from most of the points (more on what it really might mean later).\n","\n","---\n","\n","A variance of $0$ means there is no spread. If we instead take $X = [1, 1, 1, 1]$, then clearly the mean $\\bar{X} = 1$. So, $X_{centered} = [0, 0, 0, 0]$, which directly leads to a variance of $0$. (Make sure you understand why! Remember that variance is the average squared distance of each point from the mean.)"]},{"cell_type":"markdown","metadata":{"id":"Qdz4LMqYED4b"},"source":["Remember how we just calculated mean squared error to determine the accuracy of our prediction? It turns out we can do this for any statistical estimator, including mean, variance, and machine learning models.\n","\n","We can even decompose mean squared error to identify where the source of error comes from."]},{"cell_type":"markdown","metadata":{"id":"vMBtRwLxED4b"},"source":["**That can be a lot to take in, so let's break it down in Python.**\n","\n","#### Assign the first 5 rows of titanic age data to a variable:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PvIhxIQDED4c"},"outputs":[],"source":["# Take the first five rows of titanic age data.\n","first_five = titanic.age[:5]\n","\n","print(first_five)"]},{"cell_type":"markdown","metadata":{"id":"3MHVvpoAED4c"},"source":["#### Calculate the mean by hand:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yhdt8-k-ED4c"},"outputs":[],"source":["# Calculate mean by hand.\n","mean = (22 + 38 + 26 + 35 + 35) / 5.0\n","\n","mean"]},{"cell_type":"markdown","metadata":{"id":"D39yvlPOED4c"},"source":["#### Calculate the variance by hand:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m2J79WrrED4d"},"outputs":[],"source":["# Calculate variance by hand\n","(np.square(22 - mean) +\n","np.square(38 - mean) +\n","np.square(26 - mean) +\n","np.square(35 - mean) +\n","np.square(35 - mean)) / 4"]},{"cell_type":"markdown","metadata":{"id":"b3sswLCBED4d"},"source":["#### Calculate the variance and the standard deviation using Pandas:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fCe55Lc3ED4d"},"outputs":[],"source":["# Verify with Pandas\n","print(first_five.var())\n","print(first_five.std())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4yQoFqT4ED4d"},"outputs":[],"source":["titanic.age.var()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pO-d8SDHED4e"},"outputs":[],"source":["titanic.fare.var()"]},{"cell_type":"markdown","metadata":{"id":"1EP_EwPsED4e"},"source":["## Write a function that calculates the Variance\n","<img src=\"https://www.dropbox.com/scl/fi/r0b7cvcfioqbk4985hzye/hands_on.jpg?rlkey=jukxyfvobw2s86ti2qxj71vid&raw=1\" width=\"100\" height=\"100\" align=\"right\"/>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ykf1QUztED4e"},"outputs":[],"source":["# Type your answer here\n","input_list=[22 ,38 ,26 ,35 , 35]\n","import numpy as np\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nWfZ81ojED4e"},"outputs":[],"source":["def varcalc(input_list):\n","    pass\n","\n","varcalc(input_list)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8WZlgITpED4e"},"outputs":[],"source":["np.array(input_list).var(ddof=1)"]},{"cell_type":"markdown","metadata":{"id":"CapfYo6TED4f"},"source":["## Covariance and Correlation coefficient\n","https://towardsdatascience.com/essential-statistics-for-data-science-ml-4595ff07a1fa"]},{"cell_type":"markdown","metadata":{"id":"wWnBXUwkED4f"},"source":["Covariance is a measure of the joint probability of two random variables. It shows the similarity of those variables, which means that if the greater and lesser values of the one variable mainly correspond to the ones from the second variable, the covariance is positive. If the opposite happens then the covariance is negative. If it is approximately or equal to zero, the variables are independent from each other. It is often represented as cov(X, Y), σxʏ or σ(X, Y) for two variables X and Y and its formal definition is: the expected value of the product of their deviations from their individual expected values(arithmetic mean).\n"]},{"cell_type":"markdown","metadata":{"id":"xwlSsV0YED4f"},"source":["<img src=\"https://www.dropbox.com/scl/fi/wumnrcig0d11jdxlrau3i/im1.png?rlkey=bgfl9ht9wygi6rkw8v2mr5rlz&raw=1\" align=\"center\"/>"]},{"cell_type":"markdown","metadata":{"id":"FxX6STMYED4f"},"source":["By “unpacking” the outer Expected value, but with equal probabilities pᵢ between X=xᵢ and Y=yᵢ, for (i = 1,…,n), we get:"]},{"cell_type":"markdown","metadata":{"id":"_0UyJJbwED4g"},"source":["<img src=\"https://www.dropbox.com/scl/fi/g82dzm56odyycs2b8md1n/im2.png?rlkey=ik8u2y15a8bbm9n6npqgb9bf8&raw=1\"  align=\"center\"/>"]},{"cell_type":"markdown","metadata":{"id":"4UvU1xEZED4g"},"source":["and more generalized:"]},{"cell_type":"markdown","metadata":{"id":"MgxTQK82ED4g"},"source":["<img src=\"https://www.dropbox.com/scl/fi/1pzuiu6q48mveyyto8vz9/im3.png?rlkey=48zr8hy90lgygn81268p4d0xr&raw=1\"  align=\"center\"/>"]},{"cell_type":"markdown","metadata":{"id":"3iiQ4namED4h"},"source":["The above is the Population covariance. We can calculate the Sample covariance with the same rules that apply to the Sample variance. We just use the unbiased version: Take the same amount of observations of size (n) from each variable, calculate their Expected value and replace 1/n with 1/(n-1)."]},{"cell_type":"markdown","metadata":{"id":"41-d7S2NED4h"},"source":["A special case of covariance is when the two variables are identical (Covariance of a variable with itself). In that case, it is equal with the variance of that variable."]},{"cell_type":"markdown","metadata":{"id":"ZkErDtEUED4h"},"source":["<img src=\"https://www.dropbox.com/scl/fi/5xdlcrl58l8iwvlkt1ngm/im4.png?rlkey=lox8sygp6ofif5r94l4kxcmc6&raw=1\" align=\"center\"/>"]},{"cell_type":"markdown","metadata":{"id":"7Fmier6IED4i"},"source":["The above is the Population covariance. We can calculate the Sample covariance with the same rules that apply to the Sample variance. We just use the unbiased version: Take the same amount of observations of size (n) from each variable, calculate their Expected value and replace 1/n with 1/(n-1).\n","\n","\n","Now if we divide the covariance of two variables with the product of their standard deviation we will obtain the Pearson’s correlation coefficient. It is a normalization of the covariance so that it has values between +1 and -1 and it is used to make the magnitude interpretable."]},{"cell_type":"markdown","metadata":{"id":"R-oXxtQmED4i"},"source":["<img src=\"https://www.dropbox.com/scl/fi/y8rvjpna779c9edp63rk7/im5.png?rlkey=mvunc6khpzvd5rxwzvzqatrt1&raw=1\" align=\"center\"/>"]},{"cell_type":"markdown","metadata":{"id":"z5OVrV_BED4i"},"source":["## Covariance and Correlation Matrix\n","It is a square matrix that describes the covariance between two or more variables. The covariance Matrix of a random vector X is typically denoted by Kxx or Σ. For example, if we want to calculate the covariance between three variables (X, Y, Z), we must construct the matrix as follows:\n"]},{"cell_type":"markdown","metadata":{"id":"Wk3nQ1c8ED4j"},"source":["<img src=\"https://www.dropbox.com/scl/fi/gwdftrek6c7trqsdbx4h7/im6.png?rlkey=y7tgv5er5105w7auhtwm25ng6&raw=1\"  align=\"center\"/>"]},{"cell_type":"markdown","metadata":{"id":"tdJ3_0BsED4j"},"source":["Every cell is the covariance between a row variable with its corresponding column variable. As you may have noticed, the diagonal of the matrix contains the special case of the covariance(Covariance of a variable with itself) and thus represents the variance of that variable. Another thing that you may have observed is that the matrix is symmetric and the covariance values under the diagonal are the same as those over it."]},{"cell_type":"markdown","metadata":{"id":"KKv4i_EEED4j"},"source":["With the same logic, one can construct the Pearson’s correlation coefficient matrix, in which every covariance is divided by the product of the standard deviation of its corresponding variables. In that case, the diagonal always equals 1 which denotes total positive linear correlation."]},{"cell_type":"markdown","metadata":{"id":"Iojy1PdAED4j"},"source":["<a id=\"a-short-introduction-to-model-bias-and-variance\"></a>\n","## A Short Introduction to Model Bias and Variance\n","\n","---\n","\n","- **Objective:** Describe the bias and variance of statistical estimators."]},{"cell_type":"markdown","metadata":{"id":"bVbFADG_ED4k"},"source":["In simple terms, **bias** shows how accurate a model is in its predictions. (It has **low bias** if it hits the bullseye!)\n","\n","**Variance** shows how reliable a model is in its performance. (It has **low variance** if the points are predicted consistently!)\n","\n","These characteristics have important interactions, but we will save that for later.\n","\n","![Bias and Variance](./images/biasVsVarianceImage.png)"]},{"cell_type":"markdown","metadata":{"id":"DLAiBfmcED4k"},"source":["## Examples of Bias and Variance"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mDKSyL9SED4k"},"outputs":[],"source":["heights = np.random.rand(200) + 5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zxNnDVyOED4m"},"outputs":[],"source":["def plot_means(sample_size):\n","    true_mean = np.mean(heights)\n","\n","    mean_heights = []\n","    for n in range(5,sample_size):\n","        for j in range(30):\n","            mean_height = np.mean(np.random.choice(heights, n, replace=False))\n","            mean_heights.append((n, mean_height))\n","\n","    sample_height = pd.DataFrame(mean_heights, columns=['sample_size', 'height'])\n","    sample_height.plot.scatter(x='sample_size', y='height', figsize=(14, 4), alpha=0.5)\n","\n","    plt.axhline(y=true_mean, c='r')\n","    plt.title(\"The Bias and Variance of the Mean Estimator\")\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m3sSOzxEED4m"},"outputs":[],"source":["def plot_variances(sample_size):\n","    true_variance = np.var(heights)\n","\n","    var_heights = []\n","    for n in range(5,sample_size):\n","        for j in range(30):\n","            var_height1 = np.var(np.random.choice(heights, n, replace=False), ddof=0)\n","            var_height2 = np.var(np.random.choice(heights, n, replace=False), ddof=1)\n","            var_height3 = np.var(np.random.choice(heights, n, replace=False), ddof=-1)\n","            var_heights.append((n, var_height1, var_height2, var_height3))\n","\n","    sample_var = pd.DataFrame(var_heights, columns=['sample_size', 'variance1', 'variance2', 'variance3'])\n","    sample_var.plot.scatter(x='sample_size', y='variance1', figsize=(14, 3), alpha=0.5)\n","    plt.axhline(y=true_variance, c='r')\n","    plt.title(\"The Bias and Variance of the Population Variance Estimator (n)\")\n","\n","    sample_var.plot.scatter(x='sample_size', y='variance3', figsize=(14, 3), alpha=0.5)\n","    plt.axhline(y=true_variance, c='r')\n","    plt.title(\"The Bias and Variance of the Biased Sample Variance Estimator (n+1)\")\n","\n","    sample_var.plot.scatter(x='sample_size', y='variance2', figsize=(14, 3), alpha=0.5)\n","    plt.axhline(y=true_variance, c='r')\n","    plt.title(\"The Bias and Variance of the Sample Variance Estimator (n-1)\")\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"scOS3yBBED4m"},"outputs":[],"source":["interact(plot_means, sample_size=(5,200));"]},{"cell_type":"markdown","metadata":{"id":"yf1Je12kED4m"},"source":["- The red line in the chart above is the true average height, but because we don't want to ask 200 people about their height, we take a samples.\n","\n","- The blue dots show the estimate of the average height after taking a sample. To give us an idea of how sampling works, we simulate taking multiple samples.\n","\n","- The $X$ axis shows the sample size we take, while the blue dots show the likely average heights we'll conclude for a given sample size.\n","\n","- Even though the true average height is around 7 feet, a small sample may lead us to think that it's actually 6.7 or 7.3 feet.\n","\n","- Notice that the red line is in the center of our estimates. On average, we are correct and have no bias.\n","\n","- If we take a larger sample size, we get a better estimate. This means that the variance of our estimate gets smaller with larger samples sizes."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"inD2cl9HED4m"},"outputs":[],"source":["interact(plot_variances, sample_size=(5,200));"]},{"cell_type":"markdown","metadata":{"id":"0ZFAO3R3ED4m"},"source":["- Not all estimators are created equal.\n","\n","- The red lines in the charts above show the true variance of height.\n","\n","- The top graph is the population variance estimator, while the bottom graph is the sample variance estimator.\n","\n","- It's subtle, but notice that the population variance estimator is not centered on the red line. It's actually biased and consistently underestimates the true variance, especially at low sample sizes.\n","\n","- You may also notice that the scatter of the population variance estimator is smaller. That means the variance of the population variance estimator is smaller. Essentially, it's the variability of the estimator.\n","\n","- Play around with the sliders to get a good view of the graphs."]},{"cell_type":"markdown","metadata":{"id":"1wjEyJzYED4m"},"source":["<a id=\"correlation-and-association\"></a>\n","## Correlation and Association\n","---\n","\n","- **Objective:** Describe characteristics and trends in a data set using visualizations.\n","\n","Correlation measures how variables related to each other.\n","\n","Typically, we talk about the Pearson correlation coefficient — a measure of **linear** association.\n","\n","We refer to perfect correlation as **colinearity**.\n","\n","The following are a few correlation coefficients. Note that if both variables trend upward, the coefficient is positive. If one trends opposite the other, it is negative.\n","\n","It is important that you always look at your data visually — the coefficient by itself can be misleading:"]},{"cell_type":"markdown","metadata":{"id":"sGY63la4ED4m"},"source":["\n","<img src=\"https://www.dropbox.com/scl/fi/xqi3uabyrktgfp1yezanz/correlation_examples.png?rlkey=gxfj8oir3184szojijcztx1p0&raw=1\"  align=\"center\"/>"]},{"cell_type":"markdown","metadata":{"id":"3wXRhLhwED4n"},"source":["## Corrolation vs. Causation"]},{"cell_type":"markdown","metadata":{"id":"42l8x37xED4n"},"source":["<img src=\"https://www.dropbox.com/scl/fi/kl00gp8j3d54iplyd7inr/corr.png?rlkey=k79s1cia8sa1vfhz2r0z6m5o1&raw=1\"  align=\"center\"/>"]},{"cell_type":"markdown","metadata":{"id":"XeukUhIbED4n"},"source":["- Think of various examples of studies you’ve seen in the media related to food:\n","    - \"[Study links coffee consumption to decreased risk of colorectal cancer](https://news.usc.edu/97761/new-study-links-coffee-consumption-to-decreased-risk-of-colorectal-cancer/)\"\n","    - \"[Coffee does not decrease risk of colorectal cancer](http://news.cancerconnect.com/coffee-does-not-decrease-risk-of-colorectal-cancer/)\"\n","\n","There's a whole book series based on these [Spurious Correlations](http://www.tylervigen.com/spurious-correlations)."]},{"cell_type":"markdown","metadata":{"id":"wctMRP-TED4n"},"source":["<a id=\"structure-of-causal-claims\"></a>\n","### Structure of Causal Claims\n","\n","- If X happens, Y must happen.\n","- If Y happens, X must have happened.\n","  - (You need X and something else for Y to happen.)\n","- If X happens, Y will probably happen.\n","- If Y happens, X probably happened.\n","\n","> **Note:** Properties from definitions are not causal. If some a shape is a triangle, it's implied that it has three sides. However, it being a triangle does not _cause_ it to have three sides."]},{"cell_type":"markdown","metadata":{"id":"DHNueqYQED4o"},"source":["### \"Confounder\" Effect?\n","\n","Let’s say we performed an analysis to understand what causes lung cancer.\n","\n","We find that people who carry cigarette lighters are 2.4 times more likely to contract lung cancer than people who don’t carry lighters.\n","\n","Does this mean that the lighters are causing cancer?"]},{"cell_type":"markdown","metadata":{"id":"1cUIEHL7ED4o"},"source":["<img src=\"https://www.dropbox.com/scl/fi/ab74s3zqfkcpmjebz00wp/smoke.png?rlkey=z7863kq5nzvg1io629m7zumdb&raw=1\"  align=\"center\"/>"]},{"cell_type":"markdown","metadata":{"id":"pxVWlBLzED4o"},"source":["<a id=\"codealong-correlation-in-pandas\"></a>\n","## Code-Along: Correlation in Pandas\n","<img src=\"https://www.dropbox.com/scl/fi/r0b7cvcfioqbk4985hzye/hands_on.jpg?rlkey=jukxyfvobw2s86ti2qxj71vid&raw=1\" width=\"100\" height=\"100\" align=\"right\"/>\n","**Objective:** Explore options for measuring and visualizing correlation in Pandas."]},{"cell_type":"markdown","metadata":{"id":"_qn3g1j8ED4o"},"source":["#### Display the correlation matrix for all Titanic variables:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M5MiBs4jED4p"},"outputs":[],"source":["# A:\n"]},{"cell_type":"markdown","metadata":{"id":"JmTTKXNrED4p"},"source":["#### Use Seaborn to plot a heat map of the correlation matrix:\n","\n","The `sns.heatmap()` function will accomplish this.\n","\n","- Generate a correlation matrix from the Titanic data using the `.corr()` method.\n","- Pass the correlation matrix into `sns.heatmap()` as its only parameter."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KHKziCrOED4p"},"outputs":[],"source":["# Use Seaborn to plot a correlation heat map\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GbgkRcaaED4q"},"outputs":[],"source":["# Take a closer look at the survived and fare variables using a scatter plot\n","\n","\n","# Is correlation a good way to inspect the association of fare and survival?"]},{"cell_type":"markdown","metadata":{"id":"7qEQ7CgpED4q"},"source":["<a id=\"the-normal-distribution\"></a>\n","## The Normal Distribution\n","---\n","\n","- **Objective:** Identify a normal distribution within a data set using summary statistics and data visualizations."]},{"cell_type":"markdown","metadata":{"id":"kXXwPP7CED4q"},"source":["<img src=\"https://www.dropbox.com/scl/fi/kgqymdvzi0u9ncfo0vszv/normaldis.png?rlkey=z5m809ll3i1wkqjbotg4f1jtj&raw=1\"  align=\"center\"/>"]},{"cell_type":"markdown","metadata":{"id":"d2Z63gHMED4q"},"source":["###  Math Review\n","- What is an event space?\n","  - A listing of all possible occurrences.\n","- What is a probability distribution?\n","  - A function that describes how events occur in an event space.\n","- What are general properties of probability distributions?\n","  - All probabilities of an event are between 0 and 1.\n","  - The probability that something occurs is almost certain, or 1.\n","  "]},{"cell_type":"markdown","metadata":{"id":"jxJj8QrrED4r"},"source":["<a id=\"what-is-the-normal-distribution\"></a>\n","### What is the Normal Distribution?\n","- A normal distribution is often a key assumption to many models.\n","  - In practice, if the normal distribution assumption is not met, it's not the end of the world. Your model is just less efficient in most cases.\n","\n","- The normal distribution depends on the mean and the standard deviation.\n","\n","- The mean determines the center of the distribution. The standard deviation determines the height and width of the distribution.\n","\n","- Normal distributions are symmetric, bell-shaped curves.\n","\n","- When the standard deviation is large, the curve is short and wide.\n","\n","- When the standard deviation is small, the curve is tall and narrow.\n","\n","\n","<img src=\"https://www.dropbox.com/scl/fi/tpspd6vej23lvgsoa46e1/normal.png?rlkey=e6plz3t95n1e6g5xbjy1bhw70&raw=1\"  align=\"center\"/>\n"]},{"cell_type":"markdown","metadata":{"id":"2KkbCGvLED4r"},"source":["#### Why do we care about normal distributions?\n","\n","- They often show up in nature.\n","- Aggregated processes tend to distribute normally, regardless of their underlying distribution — provided that the processes are uncorrelated or weakly correlated (central limit theorem).\n","- They offer effective simplification that makes it easy to make approximations."]},{"cell_type":"markdown","metadata":{"id":"nIQgiq3aED4r"},"source":["#### Plot a histogram of 1,000 samples from a random normal distribution:\n","\n","The `np.random.randn(numsamples)` function will draw from a random normal distribution with a mean of 0 and a standard deviation of 1.\n","\n","- To plot a histogram, pass a NumPy array with 1000 samples as the only parameter to `plt.hist()`.\n","- Change the number of bins using the keyword argument `bins`, e.g. `plt.hist(mydata, bins=50)`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CkI81LwhED4r"},"outputs":[],"source":["# Plot a histogram of several random normal samples from NumPy.\n","my_array = np.random.randn(1000)\n","pd.Series(my_array).plot(kind='hist', bins=10)"]},{"cell_type":"markdown","metadata":{"id":"dcL3r96gED4r"},"source":["<a id=\"skewness\"></a>\n","###  Skewness\n","- Skewness is a measure of the asymmetry of the distribution of a random variable about its mean.\n","- Skewness can be positive or negative, or even undefined.\n","- Notice that the mean, median, and mode are the same when there is no skew.\n","<img src=\"https://www.dropbox.com/scl/fi/hnb88mg7nwfmgkicq18we/skewness-mean-median-mode.jpg?rlkey=j9nyah3bouzo59xdyzdrearmz&raw=1\" align=\"center\"/>\n"]},{"cell_type":"markdown","metadata":{"id":"8fwDJmGLED4s"},"source":["#### Plot a lognormal distribution generated with NumPy.\n","\n","Take 1,000 samples using `np.random.lognormal(size=numsamples)` and plot them on a histogram."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DaXOXh3nED4s"},"outputs":[],"source":["# Plot a lognormal distribution generated with NumPy"]},{"cell_type":"markdown","metadata":{"id":"bY8zCJ4YED4s"},"source":["#####  Real World Application - When mindfullness beats complexity\n","- Skewness is surprisingly important.\n","- Most algorithms implicitly use the mean by default when making approximations.\n","- If you know your data is heavily skewed, you may have to either transform your data or set your algorithms to work with the median."]},{"cell_type":"markdown","metadata":{"id":"fKLM_8qIED4t"},"source":["<a id=\"kurtosis\"></a>\n","### Kurtosis\n","- Kurtosis is a measure of whether the data are peaked or flat, relative to a normal distribution.\n","- Data sets with high kurtosis tend to have a distinct peak near the mean, decline rather rapidly, and have heavy tails.\n","\n","<img src=\"https://www.dropbox.com/scl/fi/plp2t7aovdz9v7xps312i/kurtosis.jpg?rlkey=0yjx93pn6jdgaqgvtykehrydx&raw=1\"  align=\"center\"/>\n"]},{"cell_type":"markdown","metadata":{"id":"MJqsipCEED4t"},"source":["####  Real-World Application: Risk Analysis\n","- Long-tailed distributions with high kurtosis elude intuition; we naturally think the event is too improbable to pay attention to.\n","- It's often the case that there is a large cost associated with a low-probability event, as is the case with hurricane damage.\n","- It's unlikely you will get hit by a Category 5 hurricane, but when you do, the damage will be catastrophic.\n","- Pay attention to what happens at the tails and whether this influences the problem at hand.\n","- In these cases, understanding the costs may be more important than understanding the risks."]},{"cell_type":"markdown","metadata":{"id":"R4JbvdAZED4t"},"source":["## More examples of Distributions\n","<img src=\"https://www.dropbox.com/scl/fi/njobu5lxphuu0sggo8s5v/distributions.png?rlkey=en56faiw7y1o97gnv753w2k6v&raw=1\"  align=\"center\"/>"]},{"cell_type":"markdown","metadata":{"id":"pqs0_3WkED4t"},"source":["<a id=\"determining-the-distribution-of-your-data\"></a>\n","## Determining the Distribution of Your Data\n","---\n","\n","**Objective:** Create basic data visualizations, including scatterplots, box plots, and histograms."]},{"cell_type":"markdown","metadata":{"id":"j371Ot-1ED4u"},"source":["![](./assets/images/distributions.png)"]},{"cell_type":"markdown","metadata":{"id":"gmV2UXr0ED4v"},"source":["#### Use the `.hist()` function of your Titantic DataFrame to plot histograms of all the variables in your data.\n","\n","- The function `plt.hist(data)` calls the Matplotlib library directly.\n","- However, each DataFrame has its own `hist()` method that by default plots one histogram per column.\n","- Given a DataFrame `my_df`, it can be called like this: `my_df.hist()`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y6JtHSmOED4v"},"outputs":[],"source":["# Plot all variables in the Titanic data set using histograms:\n","#titanic.fare.plot(kind='density')\n","titanic.fare.plot(kind='hist',bins=8)\n"]},{"cell_type":"markdown","metadata":{"id":"kR7UrFYYED4v"},"source":["#### Use the built-in `.plot.box()` function of your Titanic DataFrame to plot box plots of your variables.\n","\n","- Given a DataFrame, a box plot can be made where each column is one tick on the x axis.\n","- To do this, it can be called like this: `my_df.plot.box()`.\n","- Try using the keyword argument `showfliers`, e.g. `showfliers=False`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5DMAVnxLED4w"},"outputs":[],"source":["# Plotting all histograms can be unweildly; box plots can be more concise:\n","titanic.boxplot()"]},{"cell_type":"markdown","metadata":{"id":"QkPP9y0dED4w"},"source":["<a id=\"exercise\"></a>\n","### Exercise\n","\n","1. Look at the Titanic data variables.\n","- Are any of them normal?\n","- Are any skewed?\n","- How might this affect our modeling?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2htqR5aSED4x"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"Rj0ifqu6ED4x"},"source":["## Optional: Building  a model"]},{"cell_type":"markdown","metadata":{"id":"LG4Uc4ASED4x"},"source":["<img src=\"https://www.dropbox.com/scl/fi/lwxdpuuw7mxutk8grlnho/iris.png?rlkey=tjtbjymfa7hi4f06x86k44c5e&raw=1\"  align=\"center\"/>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zGszt7twED4x"},"outputs":[],"source":["import pandas as pd\n","from sklearn.datasets import load_iris\n","X, y = load_iris(return_X_y=True)\n","iris=load_iris(return_X_y=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8dNs5FmvED4x"},"outputs":[],"source":["X"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UwB_h0JgED4y"},"outputs":[],"source":["y"]},{"cell_type":"markdown","metadata":{"id":"fnxWeIq0ED4y"},"source":["## by using .fit() we build a model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UfjMkH9BED4y"},"outputs":[],"source":["from sklearn import tree\n","tree_model = tree.DecisionTreeClassifier()\n","tree_model.fit(X, y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A167GytOED4z"},"outputs":[],"source":["tree_model.predict([[5.4, 2, 4, 0.4]])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9X6np7dCED4z"},"outputs":[],"source":["from sklearn.datasets import load_iris\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.tree import export_text\n","iris = load_iris()\n","decision_tree = DecisionTreeClassifier(random_state=0, max_depth=5)\n","decision_tree = decision_tree.fit(iris.data, iris.target)\n","r = export_text(decision_tree, feature_names=iris['feature_names'])\n","print(r)\n"]},{"cell_type":"markdown","metadata":{"id":"FPuBucODED4z"},"source":["## Using PyDot plus for plotting the tree"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vFcT2AzUED40"},"outputs":[],"source":["# Load libraries\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn import datasets\n","from IPython.display import Image\n","from sklearn import tree\n","import pydotplus\n","\n","iris = datasets.load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Create decision tree classifer object\n","clf = DecisionTreeClassifier(random_state=0, max_depth=5)\n","\n","# Train model\n","model = clf.fit(X, y)\n","\n","# Create DOT data\n","dot_data = tree.export_graphviz(clf, out_file=None,\n","                                feature_names=iris.feature_names,\n","                                class_names=iris.target_names)\n","\n","# Draw graph\n","graph = pydotplus.graph_from_dot_data(dot_data)\n","\n","# Show graph\n","Image(graph.create_png())"]},{"cell_type":"markdown","metadata":{"id":"F5Jpvr6hED41"},"source":["## Optional : Lets review a Kaggle Challenge\n","\n","https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python"]},{"cell_type":"markdown","metadata":{"id":"TRAEoy6gED41"},"source":["<a id=\"topic-review\"></a>\n","## Lesson Review\n","---\n","\n","- We covered several different types of summary statistics, what are they?\n","- We covered three different types of visualizations, which ones?\n","- Describe bias and variance and why they are important.\n","- What are some important characteristics of distributions?\n","\n","**Any further questions?**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cn7BGY0_ED41"},"outputs":[],"source":[]}],"metadata":{"celltoolbar":"Slideshow","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}