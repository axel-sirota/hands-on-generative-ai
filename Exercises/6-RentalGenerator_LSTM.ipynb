{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "# Text Generation with a word based RNN/LSTM\n",
    "\n",
    "In this notebook we will train from scratch in an unsupervised fashion text generation to replicate the descriptions of the Airbnb rentals. For time limitations it will not be the best model, but it is structured in a portable fashion to your needs.\n",
    "\n",
    "Take it easy and pay attention to the model, how we pass states, and the complexities to do the passthrough of passing the states word by word. Also notice the differences in masking on the OneStep model!\n",
    "\n",
    "You can run this lab both locally or in Colab.\n",
    "\n",
    "- To run in Colab just go to `https://colab.research.google.com`, sign-in and you upload this notebook. Colab has GPU access for free.\n",
    "- To run locally just run `jupyter notebook` and access the notebook in this lab. You would need to first install the requirements in `requirements.txt`\n",
    "\n",
    "Follow the instructions. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIKJS_35hR8q"
   },
   "outputs": [],
   "source": [
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CnbTWB1khR5s"
   },
   "outputs": [],
   "source": [
    "!pip install textblob 'keras-nlp' 'keras-preprocessing' 'gensim==4.2.0' np_utils swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4YkshUXDhR0Y"
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Lambda, ELU, Conv1D, MaxPooling1D, Dropout, GRU, LSTM\n",
    "from keras import Model, Input\n",
    "import np_utils\n",
    "import swifter\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from textblob import TextBlob, Word\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.initializers import Constant\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "import keras_nlp\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import warnings\n",
    "import nltk\n",
    "import pickle\n",
    "from tensorflow.nn import leaky_relu\n",
    "\n",
    "import re\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from textblob import TextBlob\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "TRACE = False\n",
    "embedding_dim = 100\n",
    "rnn_units = 128\n",
    "epochs=100\n",
    "buffer_size = 256\n",
    "corpus_size=25000\n",
    "test_corpus_size=5000\n",
    "# Batch size\n",
    "batch_size = 256\n",
    "min_count_words = 3\n",
    "BATCH = True\n",
    "\n",
    "def set_seeds_and_trace():\n",
    "  os.environ['PYTHONHASHSEED'] = '0'\n",
    "  np.random.seed(42)\n",
    "  tf.random.set_seed(42)\n",
    "  random.seed(42)\n",
    "  if TRACE:\n",
    "    tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "def set_session_with_gpus_and_cores():\n",
    "  cores = multiprocessing.cpu_count()\n",
    "  gpus = len(tf.config.list_physical_devices('GPU'))\n",
    "  config = tf.compat.v1.ConfigProto( device_count = {'GPU': gpus  , 'CPU': cores} , intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "  sess = tf.compat.v1.Session(config=config)\n",
    "  tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "set_seeds_and_trace()\n",
    "set_session_with_gpus_and_cores()\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('punkt')\n",
    "textblob_tokenizer = lambda x: TextBlob(x).words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wCyoTGiHhRN-"
   },
   "outputs": [],
   "source": [
    "%%writefile get_data.sh\n",
    "if [ ! -f train_corpus_descriptions_airbnb.csv ]; then\n",
    "  wget -O train_corpus_descriptions_airbnb.csv https://www.dropbox.com/s/5rp7ibop99qyafo/train_corpus_descriptions_airbnb.csv?dl=0\n",
    "fi\n",
    "\n",
    "if [ ! -f test_corpus_descriptions_airbnb.csv ]; then\n",
    "    wget -O test_corpus_descriptions_airbnb.csv https://www.dropbox.com/s/a29bbkg8hi4q4f4/test_corpus_descriptions_airbnb.csv?dl=0\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EgyETfkigMHP"
   },
   "outputs": [],
   "source": [
    "!bash get_data.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r1amH2OpgMHQ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_path = \"./train_corpus_descriptions_airbnb.csv\"\n",
    "test_path = \"./test_corpus_descriptions_airbnb.csv\"\n",
    "# Read, then decode for py2 compat.\n",
    "airbnb_reviews = pd.read_csv(train_path, header=None, names=[\"review\"]).dropna().sample(n=corpus_size).reset_index(drop=True)\n",
    "test_airbnb_reviews = pd.read_csv(test_path, header=None, names=[\"review\"]).dropna().sample(n=test_corpus_size).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I6n1DYQMmnfG"
   },
   "outputs": [],
   "source": [
    "airbnb_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7mJb_78qgMHR",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Take a look at the first review in text\n",
    "print(airbnb_reviews.iloc[0].review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tyvG2sg_OPOi"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text, should_join=True):\n",
    "    text = str(text)\n",
    "    text = ' '.join(str(word).lower() for word in textblob_tokenizer(text))\n",
    "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
    "    if should_join:\n",
    "      return ' '.join(gensim.utils.simple_preprocess(text))\n",
    "    else:\n",
    "      return gensim.utils.simple_preprocess(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "atRfW5TgYU-x"
   },
   "outputs": [],
   "source": [
    "def get_maximum_review_length(df):\n",
    "    maximum = 0\n",
    "    for ix, row in df.iterrows():\n",
    "        candidate = len(preprocess_text(row.review, should_join=False))\n",
    "        if candidate > maximum:\n",
    "            maximum = candidate\n",
    "    return maximum\n",
    "\n",
    "\n",
    "maximum = get_maximum_review_length(airbnb_reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MQlv5nWmnqw0"
   },
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "\n",
    "# Construct the vocabulary, you are free to iterate the DF or use TF utilities.\n",
    "# What is key is that the vocab will be of words such that their frequency is above min_count_words!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CMYUrVwFgMHR"
   },
   "outputs": [],
   "source": [
    "print(f'{len(vocab)} unique words')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j9e_y27xgMHS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ids_from_words = None  # Construct the ids from words mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vq5wMDSugMHT"
   },
   "outputs": [],
   "source": [
    "words_from_ids = None  # And the inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "czQvLYzDgMHT"
   },
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "  return tf.strings.reduce_join(words_from_ids(ids), axis=-1, separator=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PdEueo3ngMHU"
   },
   "outputs": [],
   "source": [
    "ids = ids_from_words(preprocess_text('Only you can prevent forest fires', should_join=False))\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "knNacRyjgMHU"
   },
   "outputs": [],
   "source": [
    "text_from_ids(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eVm2DhrSt9bs"
   },
   "outputs": [],
   "source": [
    "def get_ids_tensor(df):\n",
    "  all_ids = tf.constant(np.zeros((1, maximum)), dtype='int64')\n",
    "  for review in df.review:\n",
    "      review = None # Preprocess the review (should it output text or a list?)\n",
    "      value = ids_from_words(review[0: maximum])\n",
    "      value = None  # if the review has fewer words than maximum pad with zeroes on the right\n",
    "      value = tf.reshape(value, [1, maximum])\n",
    "      output = None # concatenate\n",
    "      all_ids = None  # reshape\n",
    "  return all_ids[1:]\n",
    "\n",
    "all_ids = get_ids_tensor(df=airbnb_reviews)\n",
    "print(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oTyicxUO4ER7"
   },
   "outputs": [],
   "source": [
    "test_all_ids = get_ids_tensor(df=test_airbnb_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uo1Wgt5agMHV"
   },
   "outputs": [],
   "source": [
    "#Prepare the dataset\n",
    "ids_dataset = None # Create tensorflow Dataset object from all_ids tensor\n",
    "test_ids_dataset = None # Do likewise for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G6s18du1gMHW"
   },
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_YYp6t2KgMHW",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset = ids_dataset.map(split_input_target)\n",
    "test_dataset = test_ids_dataset.map(split_input_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dzym60TTgMHW",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aYRoOShmgMHX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(buffer_size)\n",
    "    .batch(batch_size=batch_size, drop_remainder=True)\n",
    ")\n",
    "test_dataset = (\n",
    "    test_dataset\n",
    "    .batch(batch_size=batch_size, drop_remainder=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6GR56NhN2-iL"
   },
   "outputs": [],
   "source": [
    "dataset.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pp6f_sr-gMHX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class RentalGenerator(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units, seq_length):\n",
    "    super().__init__(self)\n",
    "    self.vocab_size = vocab_size\n",
    "    self.seq_length = seq_length\n",
    "    self.embedding = None # Create Embedding layer, which should be its shape? If you want test out training a word2vec and set it here to fine-tune\n",
    "    self.rnn = None # Create a LSTM layer with tanh activation to ensure you can use cuDNN libraries. You should return both sequences and states\n",
    "    self.dense = None # Final Dense layer as always\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    # Implement the forward pass\n",
    "    pass\n",
    "\n",
    "  def build_graph(self):\n",
    "    x = Input(shape=(self.seq_length, ))   # doesn't consider the batch\n",
    "    Model(inputs=x, outputs=self.call(x))\n",
    "    self.build((None, self.seq_length, ))   # takes into consideration the batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uZ0cvE_YgMHY",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = RentalGenerator(\n",
    "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
    "    vocab_size=len(ids_from_words.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    "    seq_length=seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c9msjkHN6mWu",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.build_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9i87F5Fr6mWv",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-dcNCEWogMHY",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KzX4F9z-gMHY",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Compile and train the model such that it early stops if the perplexity in the val set does not decrease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9KQWnLby8G5t"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# function for plotting loss\n",
    "def plot_metrics(train_metric, val_metric=None, metric_name=None, title=None, ylim=5):\n",
    "    plt.title(title)\n",
    "    plt.ylim(0,ylim)\n",
    "    plt.plot(train_metric,color='blue',label=metric_name)\n",
    "    if val_metric is not None: plt.plot(val_metric,color='green',label='val_' + metric_name)\n",
    "    plt.legend(loc=\"upper right\")\n",
    "\n",
    "# plot loss history\n",
    "plot_metrics(history.history['loss'], val_metric=history.history['val_loss'], metric_name=\"Loss\", title=\"Loss\", ylim=5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3WQEZfLg8Hxd"
   },
   "outputs": [],
   "source": [
    "plot_metrics(history.history['perplexity'], val_metric=history.history['val_perplexity'], metric_name=\"perplexity\", title=\"perplexity\", ylim=2000.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zy0-768EgMHa",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, word_from_ids, ids_from_words, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "    self.word_from_ids = word_from_ids\n",
    "    self.ids_from_words = ids_from_words\n",
    "\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_words(['[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_words.get_vocabulary())])\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function(reduce_retracing=True)\n",
    "  def expand_dims_if_neccesary(self, input):\n",
    "    if len(input.shape) < 3:\n",
    "      input = tf.expand_dims(input, axis=0)\n",
    "    return input\n",
    "\n",
    "  @tf.function(reduce_retracing=True)\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_words = preprocess_text(inputs, should_join=False)\n",
    "    input_ids = self.expand_dims_if_neccesary(self.ids_from_words(input_words))\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_word = self.word_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_word, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lv2UPlTWgMHa",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, words_from_ids, ids_from_words)\n",
    "start = time.time()\n",
    "states = None\n",
    "description = tf.constant(['Midtown Sunny 2-Bedroom'])\n",
    "# result = [description]\n",
    "\n",
    "for n in range(100):\n",
    "  next_word, states = one_step_model.generate_one_step(description, states=states)\n",
    "  description = tf.concat([description, next_word], axis=0)\n",
    "\n",
    "\n",
    "result = tf.strings.join(description, separator=\" \")\n",
    "end = time.time()\n",
    "print(result, '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tVWZTnQegMHa"
   },
   "outputs": [],
   "source": [
    "tf.saved_model.save(one_step_model, 'lstm_rental_generator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SDj6agGJCZdh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuClass": "premium",
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}